<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MENTER | yuezi</title><meta name="author" content="yuezi"><meta name="copyright" content="yuezi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="多模态推荐系统研究总结当前领域存在的问题 数据稀疏性问题：传统推荐系统面临用户-物品交互数据稀疏的问题 模态对齐问题：不同模态(如图像、文本)的特征分布存在天然差异 信息丢失问题：现有方法在对齐不同模态时可能丢失有价值的交互信息  现有做法 使用多模态信息(如图像、文本)作为辅助信息缓解数据稀疏性 采用图卷积网络(GCN)捕捉用户和物品之间的潜在信息 使用自监督学习(SSL)来对齐不同模态的特征">
<meta property="og:type" content="article">
<meta property="og:title" content="MENTER">
<meta property="og:url" content="https://yuezi2048.github.io/2025/06/23/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/MENTER(FREEDOM%E6%94%B9%E8%BF%9B%20%E5%A2%9E%E5%BC%BAEmb)/index.html">
<meta property="og:site_name" content="yuezi">
<meta property="og:description" content="多模态推荐系统研究总结当前领域存在的问题 数据稀疏性问题：传统推荐系统面临用户-物品交互数据稀疏的问题 模态对齐问题：不同模态(如图像、文本)的特征分布存在天然差异 信息丢失问题：现有方法在对齐不同模态时可能丢失有价值的交互信息  现有做法 使用多模态信息(如图像、文本)作为辅助信息缓解数据稀疏性 采用图卷积网络(GCN)捕捉用户和物品之间的潜在信息 使用自监督学习(SSL)来对齐不同模态的特征">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yuezi2048.github.io/img/touxiang.jpg">
<meta property="article:published_time" content="2025-06-23T09:15:15.000Z">
<meta property="article:modified_time" content="2025-06-23T09:15:15.000Z">
<meta property="article:author" content="yuezi">
<meta property="article:tag" content="论文梳理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuezi2048.github.io/img/touxiang.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MENTER",
  "url": "https://yuezi2048.github.io/2025/06/23/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/MENTER(FREEDOM%E6%94%B9%E8%BF%9B%20%E5%A2%9E%E5%BC%BAEmb)/",
  "image": "https://yuezi2048.github.io/img/touxiang.jpg",
  "datePublished": "2025-06-23T09:15:15.000Z",
  "dateModified": "2025-06-23T09:15:15.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "yuezi",
      "url": "https://yuezi2048.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yuezi2048.github.io/2025/06/23/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/MENTER(FREEDOM%E6%94%B9%E8%BF%9B%20%E5%A2%9E%E5%BC%BAEmb)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?bfb48b678d82bea9f9cc9d59227767e4";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"FPH6F2TOJL","apiKey":"06d9ea26c803ba912b4d8e13404ed34d","indexName":"blog","hitsPerPage":6,"languages":{"input_placeholder":"搜索","hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MENTER',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/backgound.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">603</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">85</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">yuezi</span></a><a class="nav-page-title" href="/"><span class="site-name">MENTER</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">MENTER</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-23T09:15:15.000Z" title="发表于 2025-06-23 17:15:15">2025-06-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-23T09:15:15.000Z" title="更新于 2025-06-23 17:15:15">2025-06-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/">小论文准备</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/">论文复现总结</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/">论文梳理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="多模态推荐系统研究总结"><a href="#多模态推荐系统研究总结" class="headerlink" title="多模态推荐系统研究总结"></a>多模态推荐系统研究总结</h1><h2 id="当前领域存在的问题"><a href="#当前领域存在的问题" class="headerlink" title="当前领域存在的问题"></a>当前领域存在的问题</h2><ol>
<li>数据稀疏性问题：传统推荐系统面临用户-物品交互数据稀疏的问题</li>
<li>模态对齐问题：不同模态(如图像、文本)的特征分布存在天然差异</li>
<li>信息丢失问题：现有方法在对齐不同模态时可能丢失有价值的交互信息</li>
</ol>
<h2 id="现有做法"><a href="#现有做法" class="headerlink" title="现有做法"></a>现有做法</h2><ol>
<li>使用多模态信息(如图像、文本)作为辅助信息缓解数据稀疏性</li>
<li>采用图卷积网络(GCN)捕捉用户和物品之间的潜在信息</li>
<li>使用自监督学习(SSL)来对齐不同模态的特征</li>
</ol>
<h2 id="现有方法仍存在的问题"><a href="#现有方法仍存在的问题" class="headerlink" title="现有方法仍存在的问题"></a>现有方法仍存在的问题</h2><ol>
<li>标准模态对齐可能导致交互信息丢失</li>
<li>模态特征与ID嵌入(代表用户&#x2F;物品身份的特征)之间的距离增大</li>
<li>不同模态对融合过程的贡献不平衡</li>
</ol>
<h2 id="本文提出的方法-MENTOR"><a href="#本文提出的方法-MENTOR" class="headerlink" title="本文提出的方法(MENTOR)"></a>本文提出的方法(MENTOR)</h2><h3 id="多模态信息编码器"><a href="#多模态信息编码器" class="headerlink" title="多模态信息编码器"></a>多模态信息编码器</h3><ol>
<li>用户-物品异构图：捕捉高阶模态特定特征<ul>
<li>为每个模态构建单独的图</li>
<li>通过多层图卷积聚合邻居信息</li>
</ul>
</li>
<li>物品-物品同构图：提取物品间重要语义关系<ul>
<li>基于物品特征相似度构建KNN图</li>
<li>冻结图结构以减少计算成本</li>
</ul>
</li>
</ol>
<h3 id="多模态融合"><a href="#多模态融合" class="headerlink" title="多模态融合"></a>多模态融合</h3><ol>
<li>增强最终嵌入：结合用户-物品图和物品-物品图的信息</li>
<li>融合视觉和文本模态：使用可学习的注意力权重平衡两种模态</li>
</ol>
<h3 id="多层次跨模态对齐"><a href="#多层次跨模态对齐" class="headerlink" title="多层次跨模态对齐"></a>多层次跨模态对齐</h3><ol>
<li>标准模态对齐层：直接对齐视觉和文本模态<ul>
<li>计算均值和协方差的差异</li>
</ul>
</li>
<li>ID间接引导层：在ID模态指导下对齐视觉和文本模态<ul>
<li>保持与ID嵌入的距离</li>
</ul>
</li>
<li>ID直接对齐层：平衡模态权重并直接与ID模态对齐<ul>
<li>对齐融合模态与各单独模态</li>
<li>直接对齐融合模态与ID模态</li>
</ul>
</li>
</ol>
<h3 id="通用特征增强-可选"><a href="#通用特征增强-可选" class="headerlink" title="通用特征增强(可选)"></a>通用特征增强(可选)</h3><ol>
<li>特征掩码任务：从特征角度生成对比视图<ul>
<li>使用dropout机制掩码部分特征</li>
<li>通过MLP构建另一视图</li>
</ul>
</li>
<li>图扰动任务：从图结构角度生成对比视图<ul>
<li>向图卷积添加随机噪声</li>
<li>使用InfoNCE损失进行对比学习</li>
</ul>
</li>
</ol>
<h2 id="创新方案为何能解决问题"><a href="#创新方案为何能解决问题" class="headerlink" title="创新方案为何能解决问题"></a>创新方案为何能解决问题</h2><ol>
<li>多层次对齐保留了交互信息：通过ID引导确保模态特征不远离ID嵌入</li>
<li>平衡了模态贡献：直接对齐融合模态与ID嵌入，优化模态权重</li>
<li>提高了模型鲁棒性：可选的特征增强任务从多方面改进特征质量</li>
</ol>
<h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p>在三个公开数据集上的实验表明，MENTOR相比现有最佳方法在Recall@20指标上提升了4.59%-6.20%。</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images-ubuntu/image-20250928115225127.png" alt="image-20250928115225127"></p>
<p>代码地址：<a target="_blank" rel="noopener" href="https://github.com/Jinfeng-Xu/MENTOR">https://github.com/Jinfeng-Xu/MENTOR</a></p>
<h1 id="MENTER"><a href="#MENTER" class="headerlink" title="MENTER"></a>MENTER</h1><p>MENTOR: multi-level self-supervised learning for multimodal recommendation（AAAI2025）</p>
<p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/33408">https://ojs.aaai.org/index.php/AAAI/article/view/33408</a></p>
<p>方法：<code>heterogeneous user-item&amp;item-item graph</code>  + <code>cross-modal alignment(ID embeddings)</code> + <code>general feature enhanement（Perturbation）</code></p>
<ul>
<li>A + B’（对齐模式优化，加入了模态权重和多种对齐模式的权重） + C<ul>
<li>A：Zhou, X.; and Shen, Z. 2023. A tale of two graphs: Freezing and denoising graph structures for multimodal recommendation. In Proceedings of the 31st ACM International Conference on Multimedia, 935–943.</li>
<li>B<ul>
<li>Wei, W.; Huang, C.; Xia, L.; and Zhang, C. 2023. MultiModal Self-Supervised Learning for Recommendation. In Proceedings of the ACM Web Conference 2023, 790–800</li>
<li>Zhou, X.; Zhou, H.; Liu, Y.; Zeng, Z.; Miao, C.; Wang, P.; You, Y.; and Jiang, F. 2023c. Bootstrap latent representations for multi-modal recommendation. In Proceedings of the ACM Web Conference 2023, 845–854.</li>
</ul>
</li>
<li>C：general feature enhancement——Graph Perturbation<ul>
<li>You, Y.; Chen, T.; Sui, Y.; Chen, T.; Wang, Z.; and Shen, Y. 2020. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33: 58125823</li>
<li>Wu, J.; Wang, X.; Feng, F.; He, X.; Chen, L.; Lian, J.; and Xie, X. 2021. Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, 726–735</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250620114751098.png" alt="image-20250620114751098">	</p>
<blockquote>
<p>数据加载 → 初始化模型参数 → 构建图结构 → 前向传播 → 计算损失 → 反向传播优化 → 测试评估</p>
</blockquote>
<h2 id="上游1：图片-视觉-ID-增强Heterogeneous"><a href="#上游1：图片-视觉-ID-增强Heterogeneous" class="headerlink" title="上游1：图片&amp;视觉&amp;ID-&gt;增强Heterogeneous"></a>上游1：图片&amp;视觉&amp;ID-&gt;增强Heterogeneous</h2><p>这里以视觉模态为例，以前向传播为基点来理解模型原理</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619103819091.png" alt="image-20250619103819091">	</p>
<blockquote>
<p>原始数据处理</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619110733370.png" alt="image-20250619110733370">	</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入：用户、正/负物品节点（原始 ID，需转换为图中的全局节点 ID）</span></span><br><span class="line">    user_nodes, pos_item_nodes, neg_item_nodes = interaction[<span class="number">0</span>], interaction[<span class="number">1</span>], interaction[<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 物品节点 ID 偏移：区分用户和物品（用户节点范围 [0, n_users)，物品节点 = n_users + 原始物品 ID）</span></span><br><span class="line">    pos_item_nodes += <span class="variable language_">self</span>.n_users  </span><br><span class="line">    neg_item_nodes += <span class="variable language_">self</span>.n_users </span><br></pre></td></tr></table></figure>

<blockquote>
<p>GCN编码（对应<span style="color:red">Visual Encoder</span>）</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619110728416.png" alt="image-20250619110728416">	</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="variable language_">self</span>.v_rep, <span class="variable language_">self</span>.v_preference = <span class="variable language_">self</span>.v_gcn(<span class="variable language_">self</span>.edge_index_dropv, <span class="variable language_">self</span>.edge_index, <span class="variable language_">self</span>.v_feat)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>双图结构输入</p>
<ul>
<li><code>edge_index_dropv</code>是dropout随机丢弃，增强泛化，在GCN中定义工作的神经元</li>
<li><code>edge_index</code>是原始数据，增强节点表示，在GCN中作为补充信息（<code>数据增强</code>）</li>
<li><code>v_feat</code>是视觉特征原始特征，通过图卷积后生成高级表示，在GCN中是原始数据</li>
</ul>
</li>
<li><p>返回值</p>
<ul>
<li><code>v_rep</code>：视觉模态表示，后续与文本模态进行<code>跨模态对齐</code></li>
<li><code>v_preference</code>：视觉偏好表示，后续与文本偏好结合</li>
</ul>
</li>
</ul>
<blockquote>
<p>多维度处理得到<strong>Heterogeneous user-item</strong></p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619110722627.png" alt="image-20250619110722627">	1</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="variable language_">self</span>.v_rep, <span class="variable language_">self</span>.v_preference = <span class="variable language_">self</span>.v_gcn(<span class="variable language_">self</span>.edge_index_dropv, <span class="variable language_">self</span>.edge_index, <span class="variable language_">self</span>.v_feat)</span><br><span class="line"><span class="variable language_">self</span>.v_rep_n1, _ = <span class="variable language_">self</span>.v_gcn_n1(<span class="variable language_">self</span>.edge_index_dropv, <span class="variable language_">self</span>.edge_index, <span class="variable language_">self</span>.v_feat, perturbed=<span class="literal">True</span>)</span><br><span class="line"><span class="variable language_">self</span>.v_rep_n2, _ = <span class="variable language_">self</span>.v_gcn_n2(<span class="variable language_">self</span>.edge_index_dropv, <span class="variable language_">self</span>.edge_index, <span class="variable language_">self</span>.v_feat, perturbed=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>原始特征</li>
<li>GCN模型1加上噪声后的特征</li>
<li>GCN模型2加上噪声后的特征</li>
</ul>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619111508781.png" alt="image-20250619111508781">	</p>
<p>这三层分支模拟了三层传播，以便后续对齐损失，即得到BPR 损失</p>
<blockquote>
<p>对比学习Trick，加上噪声增加鲁棒性</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="variable language_">self</span>.v_rep_n1, _ = <span class="variable language_">self</span>.v_gcn_n1(<span class="variable language_">self</span>.edge_index_dropv, <span class="variable language_">self</span>.edge_index, <span class="variable language_">self</span>.v_feat, perturbed=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>perturbed=true</code>生成带噪声的视觉模态，为了后续的<code>对比学习</code>，对应<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_46006468/article/details/126396843">SimGCL算法</a></li>
</ul>
<h2 id="上游2：多模态拼接-增强特征"><a href="#上游2：多模态拼接-增强特征" class="headerlink" title="上游2：多模态拼接+增强特征"></a>上游2：多模态拼接+增强特征</h2><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619105512099.png" alt="image-20250619105512099">	</p>
<blockquote>
<p>准备工作：多模态拼接</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多模态拼接：视觉+文本、ID 增强、视觉增强、文本增强</span></span><br><span class="line">representation = torch.cat((<span class="variable language_">self</span>.v_rep, <span class="variable language_">self</span>.t_rep), dim=<span class="number">1</span>)  <span class="comment"># VT 融合</span></span><br><span class="line">guide_representation = torch.cat((<span class="variable language_">self</span>.id_rep, <span class="variable language_">self</span>.id_rep), dim=<span class="number">1</span>)  <span class="comment"># ID 增强</span></span><br><span class="line">v_representation = torch.cat((<span class="variable language_">self</span>.v_rep, <span class="variable language_">self</span>.v_rep), dim=<span class="number">1</span>)  <span class="comment"># 视觉增强</span></span><br><span class="line">t_representation = torch.cat((<span class="variable language_">self</span>.t_rep, <span class="variable language_">self</span>.t_rep), dim=<span class="number">1</span>)  <span class="comment"># 文本增强</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 噪声版本的多模态拼接（对比学习用）</span></span><br><span class="line">representation_n1 = torch.cat((<span class="variable language_">self</span>.v_rep_n1, <span class="variable language_">self</span>.t_rep_n1), dim=<span class="number">1</span>)</span><br><span class="line">representation_n2 = torch.cat((<span class="variable language_">self</span>.v_rep_n2, <span class="variable language_">self</span>.t_rep_n2), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度扩展：从 [B, D] → [B, D, 1]，方便后续多模态拼接</span></span><br><span class="line"><span class="variable language_">self</span>.v_rep = torch.unsqueeze(<span class="variable language_">self</span>.v_rep, <span class="number">2</span>)</span><br><span class="line"><span class="variable language_">self</span>.t_rep = torch.unsqueeze(<span class="variable language_">self</span>.t_rep, <span class="number">2</span>)</span><br><span class="line"><span class="variable language_">self</span>.id_rep = torch.unsqueeze(<span class="variable language_">self</span>.id_rep, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>用户侧（u_v、u_id、u_t 等）加入权重学习。</p>
</blockquote>
<ul>
<li>以用户侧特征为例：取得前<code>num_user</code> 个样本，通过加权的方式学习分配的权重（<code>模态注意力机制</code>），最后将三维张量压缩回二维<ul>
<li>取得num_user的用户节点（融合视频和文本特征）：<code>[total_num, feat_dim]--&gt; [num_user, feat_dim, 2]</code></li>
<li>权重学习：weight（<code> [num_user, feat_dim, 2]</code>） * user_rep（<code>[num_user, feat_dim, 2]</code>)</li>
<li>压缩：<code>[num_user, feat_dim, 2] --&gt; [num_user, 2 * feat_dim]</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户表示构建：取前 self.num_user 个样本（用户节点），拼接多模态特征并加权</span></span><br><span class="line">user_rep = torch.cat((<span class="variable language_">self</span>.v_rep[:<span class="variable language_">self</span>.num_user], <span class="variable language_">self</span>.t_rep[:<span class="variable language_">self</span>.num_user]), dim=<span class="number">2</span>)</span><br><span class="line">user_rep = <span class="variable language_">self</span>.weight_u.transpose(<span class="number">1</span>, <span class="number">2</span>) * user_rep  <span class="comment"># 模态加权（可学习权重）</span></span><br><span class="line">user_rep = torch.cat((user_rep[:, :, <span class="number">0</span>], user_rep[:, :, <span class="number">1</span>]), dim=<span class="number">1</span>)  <span class="comment"># 维度压缩</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同理构建其他视图的用户表示</span></span><br><span class="line">guide_user_rep = ...  <span class="comment"># ID增强</span></span><br><span class="line">v_user_rep = ...  <span class="comment"># 纯视觉</span></span><br><span class="line">t_user_rep = ...  <span class="comment"># 纯文本</span></span><br><span class="line">user_rep_n1 = ... <span class="comment"># 基于GCN1加入噪声的用户特征</span></span><br><span class="line">user_rep_n2 = ... <span class="comment"># 基于GCN2加入噪声的用户特征</span></span><br></pre></td></tr></table></figure>

<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619115657918.png" alt="image-20250619115657918">	</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250620120344262.png" alt="image-20250620120344262">	</p>
<blockquote>
<p>物品侧（i_v、i_id、i_t 等）融合图像特征</p>
</blockquote>
<ul>
<li>取特征的时候取用户侧剩下的即可：<code>representation[self.num_user:] </code>（包括纯ID、纯V，纯T，V&amp;T，噪声1，噪声2）</li>
<li>通过物品特征，构建<code>Item semantic graph</code>–&gt; h，同理得到（纯ID、纯V，纯T，V&amp;T，噪声1，噪声2）其他融合版本的h_guide、h_v、h_t….</li>
<li>最终的特征融合<code>Item semantic graph</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 物品表示构建：取剩余样本（物品节点），基于多视图特征 + 物品-物品图增强</span></span><br><span class="line">item_rep = representation[<span class="variable language_">self</span>.num_user:]  <span class="comment"># 基础 VT 融合</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 物品-物品图增强（对应流程图的 Item semantic graph）</span></span><br><span class="line">h = <span class="variable language_">self</span>.buildItemGraph(item_rep)  </span><br><span class="line">item_rep = item_rep + h  <span class="comment"># 融合物品语义图信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同理构建其他视图的物品表示（ID 增强、纯视觉、纯文本、噪声版）</span></span><br><span class="line"><span class="comment"># item_rep = item_rep + h</span></span><br><span class="line">item_rep_n1 = item_rep_n1 + h_n1</span><br><span class="line">item_rep_n2 = item_rep_n2 + h_n2</span><br><span class="line">guide_item_rep = guide_item_rep + h_guide</span><br><span class="line">v_item_rep = v_item_rep + h_v</span><br><span class="line">t_item_rep = t_item_rep + h_t</span><br></pre></td></tr></table></figure>

<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619120901216.png" alt="image-20250619120901216">	</p>
<h2 id="上游3：模态任务最终多头表示"><a href="#上游3：模态任务最终多头表示" class="headerlink" title="上游3：模态任务最终多头表示"></a>上游3：模态任务最终多头表示</h2><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619121329306.png" alt="image-20250619121329306">	</p>
<p>定义不同的模态任务</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建多视图的最终表示（用户+物品拼接）</span></span><br><span class="line"><span class="variable language_">self</span>.user_rep = user_rep</span><br><span class="line"><span class="variable language_">self</span>.item_rep = item_rep</span><br><span class="line"><span class="variable language_">self</span>.result_embed = torch.cat((user_rep, item_rep), dim=<span class="number">0</span>) <span class="comment"># 基础 VT 融合头</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.guide_user_rep = guide_user_rep</span><br><span class="line"><span class="variable language_">self</span>.guide_item_rep = guide_item_rep</span><br><span class="line"><span class="variable language_">self</span>.result_embed_guide = torch.cat((guide_user_rep, guide_item_rep), dim=<span class="number">0</span>) <span class="comment"># ID 增强头</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.v_user_rep = v_user_rep</span><br><span class="line"><span class="variable language_">self</span>.v_item_rep = v_item_rep</span><br><span class="line"><span class="variable language_">self</span>.result_embed_v = torch.cat((v_user_rep, v_item_rep), dim=<span class="number">0</span>)  <span class="comment"># 纯视觉头</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.t_user_rep = t_user_rep</span><br><span class="line"><span class="variable language_">self</span>.t_item_rep = t_item_rep</span><br><span class="line"><span class="variable language_">self</span>.result_embed_t = torch.cat((t_user_rep, t_item_rep), dim=<span class="number">0</span>) <span class="comment"># 纯文本头</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.user_rep_n1 = user_rep_n1</span><br><span class="line"><span class="variable language_">self</span>.item_rep_n1 = item_rep_n1</span><br><span class="line"><span class="variable language_">self</span>.result_embed_n1 = torch.cat((user_rep_n1, item_rep_n1), dim=<span class="number">0</span>) <span class="comment"># 噪声1头</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.user_rep_n2 = user_rep_n2</span><br><span class="line"><span class="variable language_">self</span>.item_rep_n2 = item_rep_n2</span><br><span class="line"><span class="variable language_">self</span>.result_embed_n2 = torch.cat((user_rep_n2, item_rep_n2), dim=<span class="number">0</span>) <span class="comment"># 噪声2头</span></span><br></pre></td></tr></table></figure>

<p>BPR打分：通过 <code>pos_scores - neg_scores</code> 构造损失$L_{bpr}$（代码里返回分数，损失在外部计算）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从多视图表示中，提取用户、正/负物品的特征</span></span><br><span class="line">user_tensor = <span class="variable language_">self</span>.result_embed[user_nodes]</span><br><span class="line">pos_item_tensor = <span class="variable language_">self</span>.result_embed[pos_item_nodes]</span><br><span class="line">neg_item_tensor = <span class="variable language_">self</span>.result_embed[neg_item_nodes]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算得分：用户-物品点积（隐式假设“得分高的物品更匹配”）</span></span><br><span class="line">pos_scores = torch.<span class="built_in">sum</span>(user_tensor * pos_item_tensor, dim=<span class="number">1</span>)</span><br><span class="line">neg_scores = torch.<span class="built_in">sum</span>(user_tensor * neg_item_tensor, dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> pos_scores, neg_scores</span><br></pre></td></tr></table></figure>



<h2 id="下游任务-calculate-loss"><a href="#下游任务-calculate-loss" class="headerlink" title="下游任务 calculate_loss"></a>下游任务 calculate_loss</h2><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619122438742.png" alt="image-20250619122438742">	</p>
<blockquote>
<p>通过上游任务计算得到<code>贝叶斯损失</code>（基础推荐排序损失）$L_{bpr}$（<code>loss_value</code>）</p>
<ul>
<li>让正样本得分（用户与喜欢物品的交互）高于负样本得分（用户与不喜欢物品的交互）</li>
<li>通过 sigmoid 将得分差转换为概率，用对数损失优化排序的置信度</li>
</ul>
<p><code>reg_loss</code>为<code>L2正则项</code>防止过拟合</p>
<ul>
<li>对模态偏好向量（<code>v_preference</code>&#x2F;<code>t_preference</code>）进行 L2 惩罚</li>
<li>对模态加权矩阵（<code>weight_u</code>）进行 L2 惩罚</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user = interaction[<span class="number">0</span>]</span><br><span class="line">    pos_scores, neg_scores = <span class="variable language_">self</span>.forward(interaction)</span><br><span class="line">    loss_value = -torch.mean(torch.log2(torch.sigmoid(pos_scores - neg_scores)))</span><br><span class="line">    <span class="comment"># reg</span></span><br><span class="line">    reg_embedding_loss_v = (<span class="variable language_">self</span>.v_preference[user] ** <span class="number">2</span>).mean() <span class="keyword">if</span> <span class="variable language_">self</span>.v_preference <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">    reg_embedding_loss_t = (<span class="variable language_">self</span>.t_preference[user] ** <span class="number">2</span>).mean() <span class="keyword">if</span> <span class="variable language_">self</span>.t_preference <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">    reg_loss = <span class="variable language_">self</span>.reg_weight * (reg_embedding_loss_v + reg_embedding_loss_t)</span><br><span class="line">    reg_loss += <span class="variable language_">self</span>.reg_weight * (<span class="variable language_">self</span>.weight_u ** <span class="number">2</span>).mean()</span><br></pre></td></tr></table></figure>



<blockquote>
<p>$L_{enhance_f}$（<code>mask_f_loss</code>）对应特征掩码损失，属于特征的自监督算法（掩码预测）</p>
<ul>
<li>对用户 &#x2F; 物品特征进行两种变换：<ul>
<li><code>u_temp</code>：Dropout 掩码（破坏特征）</li>
<li><code>u_temp2</code>：MLP 变换（重构特征）</li>
</ul>
</li>
<li>要求变换后的特征与原始特征保持余弦相似性</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask_loss_u = <span class="number">1</span> - F.cosine_similarity(u_temp, u_temp2).mean()</span><br><span class="line">mask_loss_i = <span class="number">1</span> - F.cosine_similarity(i_temp, i_temp2).mean()</span><br><span class="line">mask_f_loss = <span class="variable language_">self</span>.mask_weight_f * (mask_loss_i + mask_loss_u)</span><br></pre></td></tr></table></figure>

<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619122829820.png" alt="image-20250619122829820">	</p>
<blockquote>
<p>$L_{enhance_f}$（align_loss）分布对齐损失，来保证不同模态特征尽量在同一语义空间</p>
<ul>
<li>计算以下特征分布的均值和方差差：<ul>
<li>ID 增强特征（g）与其他模态（r&#x2F;v&#x2F;t）</li>
<li>跨模态特征（r&#x2F;v&#x2F;t 之间）</li>
</ul>
</li>
<li>通过 L1 距离对齐不同模态的特征分布</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">align_loss = ((torch.<span class="built_in">abs</span>(g_var - r_var) + torch.<span class="built_in">abs</span>(g_mean - r_mean)).mean() +</span><br><span class="line">              (torch.<span class="built_in">abs</span>(g_var - v_var) + torch.<span class="built_in">abs</span>(g_mean - v_mean)).mean() +</span><br><span class="line">              (torch.<span class="built_in">abs</span>(g_var - t_var) + torch.<span class="built_in">abs</span>(g_mean - t_mean)).mean() +</span><br><span class="line">              (torch.<span class="built_in">abs</span>(r_var - v_var) + torch.<span class="built_in">abs</span>(r_mean - v_mean)).mean() +</span><br><span class="line">              (torch.<span class="built_in">abs</span>(r_var - t_var) + torch.<span class="built_in">abs</span>(r_mean - t_mean)).mean() +</span><br><span class="line">              (torch.<span class="built_in">abs</span>(v_var - t_var) + torch.<span class="built_in">abs</span>(v_mean - t_mean)).mean())</span><br></pre></td></tr></table></figure>

<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619123445306.png" alt="image-20250619123445306">	</p>
<blockquote>
<p>$L_{enhance_g}$（mask_g_loss） 图扰动损失</p>
<ul>
<li><strong>任务</strong>：图对比学习（SimGCL 思想）</li>
<li>逻辑<ul>
<li>对图结构进行两次随机扰动（<code>result_embed_n1</code>和<code>result_embed_n2</code>）</li>
<li>使用 InfoNCE 损失对比同一节点的不同扰动表示：<ul>
<li>同一节点的不同扰动视为正样本</li>
<li>不同节点的扰动视为负样本</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask_g_loss = (<span class="variable language_">self</span>.InfoNCE(<span class="variable language_">self</span>.result_embed_n1[:<span class="variable language_">self</span>.n_users], <span class="variable language_">self</span>.result_embed_n2[:<span class="variable language_">self</span>.n_users], <span class="variable language_">self</span>.temp)</span><br><span class="line">               + <span class="variable language_">self</span>.InfoNCE(<span class="variable language_">self</span>.result_embed_n1[<span class="variable language_">self</span>.n_users:], <span class="variable language_">self</span>.result_embed_n2[<span class="variable language_">self</span>.n_users:], <span class="variable language_">self</span>.temp))</span><br></pre></td></tr></table></figure>

<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619123452369.png" alt="image-20250619123452369">	</p>
<p>最终下游任务代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_loss</span>(<span class="params">self, interaction</span>):</span><br><span class="line">    user = interaction[<span class="number">0</span>]</span><br><span class="line">    pos_scores, neg_scores = <span class="variable language_">self</span>.forward(interaction)</span><br><span class="line">    loss_value = -torch.mean(torch.log2(torch.sigmoid(pos_scores - neg_scores)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reg</span></span><br><span class="line">    reg_embedding_loss_v = (<span class="variable language_">self</span>.v_preference[user] ** <span class="number">2</span>).mean() <span class="keyword">if</span> <span class="variable language_">self</span>.v_preference <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">    reg_embedding_loss_t = (<span class="variable language_">self</span>.t_preference[user] ** <span class="number">2</span>).mean() <span class="keyword">if</span> <span class="variable language_">self</span>.t_preference <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">    reg_loss = <span class="variable language_">self</span>.reg_weight * (reg_embedding_loss_v + reg_embedding_loss_t)</span><br><span class="line">    reg_loss += <span class="variable language_">self</span>.reg_weight * (<span class="variable language_">self</span>.weight_u ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mask</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        u_temp, i_temp = <span class="variable language_">self</span>.user_rep.clone(), <span class="variable language_">self</span>.item_rep.clone()</span><br><span class="line">        u_temp2, i_temp2 = <span class="variable language_">self</span>.user_rep.clone(), <span class="variable language_">self</span>.item_rep.clone()</span><br><span class="line">        u_temp.detach()</span><br><span class="line">        i_temp.detach()</span><br><span class="line">        u_temp2.detach()</span><br><span class="line">        i_temp2.detach()</span><br><span class="line">        u_temp2 = <span class="variable language_">self</span>.mlp(u_temp2)</span><br><span class="line">        i_temp2 = <span class="variable language_">self</span>.mlp(i_temp2)</span><br><span class="line">        u_temp = F.dropout(u_temp, <span class="variable language_">self</span>.dropout)</span><br><span class="line">        i_temp = F.dropout(i_temp, <span class="variable language_">self</span>.dropout)</span><br><span class="line">    mask_loss_u = <span class="number">1</span> - F.cosine_similarity(u_temp, u_temp2).mean()</span><br><span class="line">    mask_loss_i = <span class="number">1</span> - F.cosine_similarity(i_temp, i_temp2).mean()</span><br><span class="line">    mask_f_loss = <span class="variable language_">self</span>.mask_weight_f * (mask_loss_i + mask_loss_u)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># guide</span></span><br><span class="line">    <span class="comment"># 粗粒度-分布</span></span><br><span class="line">    r_var, r_mean, g_var, g_mean, v_var, v_mean, t_var, t_mean = <span class="variable language_">self</span>.fit_Gaussian_dis()</span><br><span class="line">    <span class="comment"># id and v+t</span></span><br><span class="line">    <span class="comment"># dis_loss_i_vt = (torch.abs(g_var - r_var) +</span></span><br><span class="line">    <span class="comment">#                  torch.abs(g_mean - r_mean)).mean()</span></span><br><span class="line">    <span class="comment"># # id and v</span></span><br><span class="line">    <span class="comment"># dis_loss_i_v = (torch.abs(g_var - v_var) +</span></span><br><span class="line">    <span class="comment">#                 torch.abs(g_mean - v_mean)).mean()</span></span><br><span class="line">    <span class="comment"># # id and t</span></span><br><span class="line">    <span class="comment"># dis_loss_i_t = (torch.abs(g_var - t_var) +</span></span><br><span class="line">    <span class="comment">#                 torch.abs(g_mean - t_mean)).mean()</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # v and v+t</span></span><br><span class="line">    <span class="comment"># dis_loss_v_vt = (torch.abs(r_var - v_var) +</span></span><br><span class="line">    <span class="comment">#                  torch.abs(r_mean - v_mean)).mean()</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # t and v+t</span></span><br><span class="line">    <span class="comment"># dis_loss_t_vt = (torch.abs(r_var - t_var) +</span></span><br><span class="line">    <span class="comment">#                  torch.abs(r_mean - t_mean)).mean()</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # v and t</span></span><br><span class="line">    <span class="comment"># dis_loss_v_t = (torch.abs(v_var - t_var) +</span></span><br><span class="line">    <span class="comment">#                 torch.abs(v_mean - t_mean)).mean()</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # total</span></span><br><span class="line">    <span class="comment"># dis_loss = (dis_loss_i_vt + dis_loss_i_v + dis_loss_i_t</span></span><br><span class="line">    <span class="comment">#             + dis_loss_v_vt + dis_loss_t_vt</span></span><br><span class="line">    <span class="comment">#             + dis_loss_v_t)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># level4</span></span><br><span class="line">    <span class="comment"># dis_loss = (dis_loss_v_t)</span></span><br><span class="line">    <span class="comment"># level3</span></span><br><span class="line">    <span class="comment"># dis_loss = (dis_loss_v_vt + dis_loss_t_vt)</span></span><br><span class="line">    <span class="comment"># level2</span></span><br><span class="line">    <span class="comment"># dis_loss = (dis_loss_i_v + dis_loss_i_t)</span></span><br><span class="line">    <span class="comment"># level1</span></span><br><span class="line">    <span class="comment"># dis_loss = dis_loss_i_vt</span></span><br><span class="line"></span><br><span class="line">    align_loss = ((torch.<span class="built_in">abs</span>(g_var - r_var) +</span><br><span class="line">                     torch.<span class="built_in">abs</span>(g_mean - r_mean)).mean() +</span><br><span class="line">                    (torch.<span class="built_in">abs</span>(g_var - v_var) +</span><br><span class="line">                     torch.<span class="built_in">abs</span>(g_mean - v_mean)).mean() +</span><br><span class="line">                   (torch.<span class="built_in">abs</span>(g_var - t_var) +</span><br><span class="line">                    torch.<span class="built_in">abs</span>(g_mean - t_mean)).mean() +</span><br><span class="line">                  (torch.<span class="built_in">abs</span>(r_var - v_var) +</span><br><span class="line">                   torch.<span class="built_in">abs</span>(r_mean - v_mean)).mean() +</span><br><span class="line">                 (torch.<span class="built_in">abs</span>(r_var - t_var) +</span><br><span class="line">                  torch.<span class="built_in">abs</span>(r_mean - t_mean)).mean() +</span><br><span class="line">                (torch.<span class="built_in">abs</span>(v_var - t_var) +</span><br><span class="line">                 torch.<span class="built_in">abs</span>(v_mean - t_mean)).mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 图噪音cl</span></span><br><span class="line">    <span class="comment"># inspired by SimGCL</span></span><br><span class="line">    mask_g_loss = (<span class="variable language_">self</span>.InfoNCE(<span class="variable language_">self</span>.result_embed_n1[:<span class="variable language_">self</span>.n_users], <span class="variable language_">self</span>.result_embed_n2[:<span class="variable language_">self</span>.n_users], <span class="variable language_">self</span>.temp)</span><br><span class="line">                   + <span class="variable language_">self</span>.InfoNCE(<span class="variable language_">self</span>.result_embed_n1[<span class="variable language_">self</span>.n_users:], <span class="variable language_">self</span>.result_embed_n2[<span class="variable language_">self</span>.n_users:], <span class="variable language_">self</span>.temp))</span><br><span class="line"></span><br><span class="line">    mask_g_loss = mask_g_loss * <span class="variable language_">self</span>.mask_weight_g</span><br><span class="line"></span><br><span class="line">    align_loss = align_loss * <span class="variable language_">self</span>.align_weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss_value + reg_loss + align_loss + mask_f_loss + mask_g_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return loss_value + reg_loss</span></span><br></pre></td></tr></table></figure>



<h2 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h2><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250619121653288.png" alt="image-20250619121653288"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yuezi2048.github.io">yuezi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yuezi2048.github.io/2025/06/23/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/MENTER(FREEDOM%E6%94%B9%E8%BF%9B%20%E5%A2%9E%E5%BC%BAEmb)/">https://yuezi2048.github.io/2025/06/23/2.Areas🌐/science/小论文准备/论文复现总结/论文梳理/MENTER(FREEDOM改进 增强Emb)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://yuezi2048.github.io" target="_blank">yuezi</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/">论文梳理</a></div><div class="post-share"><div class="social-share" data-image="/img/touxiang.jpg" data-sites="qq,wechat"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/06/21/2.Areas%F0%9F%8C%90/03.algorithm/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95/8.%E5%9B%9E%E6%BA%AF/" title="8.回溯"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">8.回溯</div></div><div class="info-2"><div class="info-item-1">八、回溯8.1 理论基础	    回溯理解回溯法，一般可以解决如下几种问题（组合无序，排列有序）：  组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等  回溯法可理解为树形结构：  集合的大小就构成了树的宽度，递归的深度就构成了树的深度 有递归那么一定有终止条件，那么必然就是高度有限的n叉树  回溯模板void backtracking(参数) &#123;    if (终止条件) &#123;        存放结果;        return;    &#125;    for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) &#123;        处理节点;        backtracking(路径，选择列表); // 递归        回溯，撤销处理结果   ...</div></div></div></a><a class="pagination-related" href="/2025/06/25/2.Areas%F0%9F%8C%90/science/Pre/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/GNN/GNN/" title="GNN"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">GNN</div></div><div class="info-2"><div class="info-item-1">为什么要重点学图？图神经网络、多模态是未来火的趋势！图神经网络入门到精通：GCN、GAT、PyG、GTN一口气学爽GNN原理及论文！ GNN在推荐系统的应用点和点之间的关系来进行推荐  图的基本定义图神经网络分为三种特征  Vertex（点）：特征 Edge（边）：关系（也会有一个特征向量） Global：整个图本身   图神经网络的核心任务：借助图的方式，做好特征（embedding）  任务：点、边、图的分类和回归   我们考虑相邻的点的关系，定义邻接矩阵：表示一个点和哪些点相连，表示邻居之间的关系  GNN(A, X)模型参数：点的特征 + 邻接矩阵   文本数据也可以存成图的形式  不同于CV和NLP，在交通、分子等领域中，输入数据规则不同 实际Pytorch任务的时候，保存的是source、target的2*N形式，而不是N*N的矩阵  消息传递  每个点的特征更新需要考虑邻居的特征  GNN也可以做多次，输入1 –&gt; 输出1（输入2）—&gt;...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/21/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/CHOESION(%E6%97%A9%E6%99%9A%E6%9C%9F%E5%8F%8C%E9%98%B6%E6%AE%B5%E8%9E%8D%E5%90%88+%E5%A4%8D%E5%90%88GCN)/" title="CHOESION(早晚期双阶段融合+复合GCN)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="info-item-2">CHOESION(早晚期双阶段融合+复合GCN)</div></div><div class="info-2"><div class="info-item-1">多模态推荐系统研究总结当前领域存在的问题 模态融合效果不佳：现有的多模态推荐系统在融合不同模态信息时，通常采用简单的注意力机制或预定义的策略，无法有效处理不同模态之间的无关信息。  表示学习受限：虽然已有研究构建了用户-物品、用户-用户和物品-物品的图结构来学习表示，但模态融合和表示学习通常被视为两个独立的过程，未能充分利用它们之间的互补关系。   现有做法 模态融合方面：  早期融合：在构建异构图之前就将模态特征与ID嵌入融合 晚期融合：先学习每个模态的表示，最后再融合所有预测评分   表示学习方面：  构建用户-物品二分图并使用图卷积网络(GCN)增强表示学习 一些工作尝试构建用户-用户图或物品-物品图来探索隐藏关系    现有做法仍存在的问题 预定义的融合策略对模态间无关信息的负面影响较为脆弱 模态融合和表示学习被视为独立过程，未能充分利用它们的互补关系 简单的图结构设计可能无法充分捕捉用户和物品之间的复杂关系  本文提出的方法(COHESION)​	 1. 双阶段融合策略(Dual-Stage...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/BeFA(%E8%A1%8C%E4%B8%BA%E5%BC%95%E5%AF%BC%E7%9A%84%E7%89%B9%E5%BE%81%E9%87%8D%E6%9E%84)/" title="BeFA(行为引导的特征重构)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">BeFA(行为引导的特征重构)</div></div><div class="info-2"><div class="info-item-1">科研小白友好版论文总结当前领域存在的问题多媒体推荐系统通常使用预训练的特征编码器提取内容特征，但这些编码器会同时提取内容中的所有信息(包括大量与用户偏好无关的细节)，导致提取的特征可能无法准确反映用户偏好。 现有做法及其局限性现有方法通常：  使用预训练编码器提取内容特征 将这些内容特征与行为特征融合来建模用户偏好  局限性：预提取的内容特征存在”信息漂移”(包含错误&#x2F;无关信息)和”信息遗漏”(缺少关键信息)问题，影响推荐准确性。  futhermore, author introduce a similarity-based visual attribution method, which enables researchers to visually analyze the quality of content features for the first...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/CMDL(%E6%A8%A1%E6%80%81%E8%A7%A3%E8%80%A6%E5%85%B1%E4%BA%AB%E5%92%8C%E7%89%B9%E6%9C%89+%E5%9F%BA%E4%BA%8EMI%E6%AD%A3%E5%88%99%E5%8C%96)/" title="CMDL(模态解耦共享和特有+基于MI正则化)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">CMDL(模态解耦共享和特有+基于MI正则化)</div></div><div class="info-2"><div class="info-item-1">论文总结：对比性模态解耦学习用于多模态推荐当前领域存在的问题现有的多模态推荐方法主要关注设计强大的编码器来提取多模态特征，并简单地聚合这些特征进行预测。这些方法在捕捉跨模态知识（包括模态共享知识和模态特有知识）方面能力有限。实际上：  学习模态共享知识可以帮助对齐跨模态数据，融合异构模态特征 学习模态特有知识同样重要，特别是当推荐任务只涉及少量共享特征时，必要信息可能包含在特定模态中  现有方法及其局限性大多数现有方法（如MMGCN、LATTICE等）：  专注于设计强大的特征编码器 简单聚合不同模态学习到的特征 无法有效区分和捕捉模态共享和模态特有知识  现有方法的主要局限性是缺乏有效的统计约束来确保：  解耦后的表示在统计上相互独立 模态不变表示能尽可能多地捕捉跨模态共享信息  本文提出的方法：CMDL	 本文提出了对比性模态解耦学习(CMDL)框架，包含以下关键创新： 1. 多模态特征预处理(AA)基准模型：MGCN’...</div></div></div></a><a class="pagination-related" href="/2025/09/09/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/DA_MRS/" title="DA_MRS"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-09</div><div class="info-item-2">DA_MRS</div></div><div class="info-2"><div class="info-item-1">当前领域存在的问题多模态推荐系统（MRS）面临三个主要问题：    多模态内容噪声：物品的图片、文本等描述可能包含无关信息（例如商品图片背景干扰或文本描述冗长），导致物品相似性计算不准确。   用户反馈噪声：用户点击、浏览等行为可能包含误操作或偏见（例如随机点击或受广告影响），误导模型学习。   多模态内容与用户反馈的对齐不足：现有方法仅通过对比学习对齐不同模态的表示，但忽略了用户偏好视角的细粒度对齐（例如相似物品的层级关系）。   现有方法现有方法主要分为两类：    基于特征的方法（如VBPR、MMGCN）：直接融合多模态特征增强物品表示，但未显式处理噪声。   基于结构的方法（如LATTICE、MICRO）：通过多模态内容构建物品相似图，但存在以下问题：   未过滤跨模态不一致的相似性（例如图片相似但文本无关的物品被错误连接）。   未利用用户行为数据补充物品关系。   对齐方法仅停留在物品级别，未考虑用户偏好和细粒度相似性。     本文提出的方法：DA-MRSDA-MRS包含三个核心模块，分别解决上述问题：   1. 去噪物品-物品图（Denoising...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/DGMRec(%E6%A8%A1%E6%80%81%E8%A7%A3%E8%80%A6%E5%85%B1%E4%BA%AB%E5%92%8C%E7%89%B9%E6%9C%89+%E6%A8%A1%E6%80%81%E7%94%9F%E6%88%90)/" title="DGMRec(模态解耦共享和特有+模态生成)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">DGMRec(模态解耦共享和特有+模态生成)</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2025/07/21/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/DGVAE(FREEDOM+DGVAE+MIM)/" title="DGVAE(FREEDOM+DGVAE+MIM)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="info-item-2">DGVAE(FREEDOM+DGVAE+MIM)</div></div><div class="info-2"><div class="info-item-1">多模态推荐系统可解释性研究总结当前领域存在的问题当前多模态推荐系统虽然能结合文本、图像等多种信息提高推荐准确性，但存在一个关键问题：这些系统使用复杂的数字向量来表示用户和物品，导致推荐结果难以解释。用户无法理解为什么系统会推荐某个物品，降低了系统的可信度和实用性。 现有做法现有方法主要通过以下几种方式处理多模态信息：  简单拼接或求和：将预处理后的多模态信息与可学习的物品嵌入向量结合 注意力机制：更精确地捕捉用户对物品的偏好 图神经网络：利用图结构捕捉物品多模态特征与用户-物品交互间的高阶关系  但这些方法都依赖数字嵌入表示用户和物品，缺乏可解释性。 现有做法仍存在的问题尽管现有方法在推荐准确性上表现不错，但存在以下问题：  可解释性差：用户无法理解推荐背后的原因 工业应用受限：缺乏解释性阻碍了在实际业务中的应用 信任度低：用户对”黑箱”推荐缺乏信任  本文提出的方法：DGVAE本文提出了一种解耦图变分自编码器(DGVAE)，主要包含以下创新点： 		 1....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">yuezi</div><div class="author-info-description">积微者速成</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">603</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">85</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yuezi2048"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://githubfast.com/yuezi2048" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_46345703" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="https://qm.qq.com/cgi-bin/qm/qr?k=dD62GIPTf5-iS4UdSfJRy7NHwsrCh3-j" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:joyLing@stumail.nwu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">积微者速成</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E6%80%BB%E7%BB%93"><span class="toc-text">多模态推荐系统研究总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E9%A2%86%E5%9F%9F%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">当前领域存在的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E5%81%9A%E6%B3%95"><span class="toc-text">现有做法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E6%96%B9%E6%B3%95%E4%BB%8D%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">现有方法仍存在的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95-MENTOR"><span class="toc-text">本文提出的方法(MENTOR)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-text">多模态信息编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88"><span class="toc-text">多模态融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%AC%A1%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AF%B9%E9%BD%90"><span class="toc-text">多层次跨模态对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E7%89%B9%E5%BE%81%E5%A2%9E%E5%BC%BA-%E5%8F%AF%E9%80%89"><span class="toc-text">通用特征增强(可选)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E6%96%B9%E6%A1%88%E4%B8%BA%E4%BD%95%E8%83%BD%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98"><span class="toc-text">创新方案为何能解决问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C"><span class="toc-text">实验效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MENTER"><span class="toc-text">MENTER</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8A%E6%B8%B81%EF%BC%9A%E5%9B%BE%E7%89%87-%E8%A7%86%E8%A7%89-ID-%E5%A2%9E%E5%BC%BAHeterogeneous"><span class="toc-text">上游1：图片&amp;视觉&amp;ID-&gt;增强Heterogeneous</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8A%E6%B8%B82%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8B%BC%E6%8E%A5-%E5%A2%9E%E5%BC%BA%E7%89%B9%E5%BE%81"><span class="toc-text">上游2：多模态拼接+增强特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8A%E6%B8%B83%EF%BC%9A%E6%A8%A1%E6%80%81%E4%BB%BB%E5%8A%A1%E6%9C%80%E7%BB%88%E5%A4%9A%E5%A4%B4%E8%A1%A8%E7%A4%BA"><span class="toc-text">上游3：模态任务最终多头表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-calculate-loss"><span class="toc-text">下游任务 calculate_loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%85%E5%8A%A9%E5%87%BD%E6%95%B0"><span class="toc-text">辅助函数</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/Excalidraw/Drawing%202024-11-01%2015.37.37.excalidraw/" title="Drawing 2024-11-01 15.37.37.excalidraw">Drawing 2024-11-01 15.37.37.excalidraw</a><time datetime="2025-10-31T04:19:26.621Z" title="发表于 2025-10-31 12:19:26">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/01.project/condefather/yupao/back-end/7.%E4%BC%98%E5%8C%96%E4%B8%BB%E9%A1%B5%E6%80%A7%E8%83%BD-%E4%BC%98%E5%8C%96%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE/" title="7.优化主页性能-优化批量导入数据">7.优化主页性能-优化批量导入数据</a><time datetime="2025-10-31T04:19:26.102Z" title="发表于 2025-10-31 12:19:26">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/02.%E9%9D%A2%E7%BB%8F%E8%AE%B0%E5%BD%95/MySQL/81.%E4%BB%80%E4%B9%88%E6%98%AF%E6%B8%B8%E6%A0%87Cursor%E5%88%86%E9%A1%B5_%E5%AF%B9%E6%AF%94%E4%BC%A0%E7%BB%9FLIMIT_OFFSET%E5%88%86%E9%A1%B5%E7%9A%84%E4%BC%98%E5%8A%BF%E6%98%AF%E4%BB%80%E4%B9%88/" title="81.什么是游标Cursor分页_对比传统LIMIT_OFFSET分页的优势是什么">81.什么是游标Cursor分页_对比传统LIMIT_OFFSET分页的优势是什么</a><time datetime="2025-10-31T04:14:27.000Z" title="发表于 2025-10-31 12:14:27">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/02.%E9%9D%A2%E7%BB%8F%E8%AE%B0%E5%BD%95/MySQL/80.%E5%85%A8%E9%87%8F%E5%90%8C%E6%AD%A5%E5%92%8C%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5%E5%90%84%E8%87%AA%E4%BC%98%E7%BC%BA%E7%82%B9/" title="80.全量同步和增量同步各自优缺点">80.全量同步和增量同步各自优缺点</a><time datetime="2025-10-31T04:13:32.000Z" title="发表于 2025-10-31 12:13:32">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/02.%E9%9D%A2%E7%BB%8F%E8%AE%B0%E5%BD%95/MySQL/79.%E4%BB%80%E4%B9%88%E6%98%AF%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE%E5%85%A5%E5%BA%93_%E7%9B%B8%E6%AF%94%E5%8D%95%E6%9D%A1%E6%8F%92%E5%85%A5%E4%BC%98%E5%8A%BF/" title="79.什么是批量数据入库_相比单条插入优势">79.什么是批量数据入库_相比单条插入优势</a><time datetime="2025-10-31T04:03:26.000Z" title="发表于 2025-10-31 12:03:26">2025-10-31</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/default.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By yuezi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>