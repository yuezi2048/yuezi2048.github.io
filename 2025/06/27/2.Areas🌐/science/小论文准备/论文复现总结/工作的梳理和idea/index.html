<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>工作的梳理和idea | yuezi</title><meta name="author" content="yuezi"><meta name="copyright" content="yuezi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、多模态推荐目前的工作1. RS大方向的问题 RS：预测用户对各种项目的评级或偏好，并根据历史交互数据和公开信息推荐最有可能且相关的项目 基于ID的传统做法：倾向选择以前交互过的物品。 问题：需要大量交互数据，那么就有数据稀疏性问题，且预测结果与ID具备强依赖关系（尤其在动态环境下），因此存在冷启动问题    2. MRS小方向的问题小方向：基于MRS充分利用辅助多模式信息补充历史交互信息，增强">
<meta property="og:type" content="article">
<meta property="og:title" content="工作的梳理和idea">
<meta property="og:url" content="https://yuezi2048.github.io/2025/06/27/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%A2%B3%E7%90%86%E5%92%8Cidea/index.html">
<meta property="og:site_name" content="yuezi">
<meta property="og:description" content="一、多模态推荐目前的工作1. RS大方向的问题 RS：预测用户对各种项目的评级或偏好，并根据历史交互数据和公开信息推荐最有可能且相关的项目 基于ID的传统做法：倾向选择以前交互过的物品。 问题：需要大量交互数据，那么就有数据稀疏性问题，且预测结果与ID具备强依赖关系（尤其在动态环境下），因此存在冷启动问题    2. MRS小方向的问题小方向：基于MRS充分利用辅助多模式信息补充历史交互信息，增强">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yuezi2048.github.io/img/touxiang.jpg">
<meta property="article:published_time" content="2025-06-27T03:56:51.000Z">
<meta property="article:modified_time" content="2025-06-27T03:56:51.000Z">
<meta property="article:author" content="yuezi">
<meta property="article:tag" content="论文复现总结">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuezi2048.github.io/img/touxiang.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "工作的梳理和idea",
  "url": "https://yuezi2048.github.io/2025/06/27/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%A2%B3%E7%90%86%E5%92%8Cidea/",
  "image": "https://yuezi2048.github.io/img/touxiang.jpg",
  "datePublished": "2025-06-27T03:56:51.000Z",
  "dateModified": "2025-06-27T03:56:51.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "yuezi",
      "url": "https://yuezi2048.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yuezi2048.github.io/2025/06/27/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%A2%B3%E7%90%86%E5%92%8Cidea/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?bfb48b678d82bea9f9cc9d59227767e4";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"FPH6F2TOJL","apiKey":"06d9ea26c803ba912b4d8e13404ed34d","indexName":"blog","hitsPerPage":6,"languages":{"input_placeholder":"搜索","hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '工作的梳理和idea',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/backgound.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">785</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">66</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">91</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">yuezi</span></a><a class="nav-page-title" href="/"><span class="site-name">工作的梳理和idea</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">工作的梳理和idea</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-27T03:56:51.000Z" title="发表于 2025-06-27 11:56:51">2025-06-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-27T03:56:51.000Z" title="更新于 2025-06-27 11:56:51">2025-06-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/">小论文准备</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/2-Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/">论文复现总结</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="一、多模态推荐目前的工作"><a href="#一、多模态推荐目前的工作" class="headerlink" title="一、多模态推荐目前的工作"></a>一、多模态推荐目前的工作</h2><h3 id="1-RS大方向的问题"><a href="#1-RS大方向的问题" class="headerlink" title="1. RS大方向的问题"></a>1. RS大方向的问题</h3><ul>
<li>RS：预测用户对各种项目的评级或偏好，并根据历史交互数据和公开信息推荐最有可能且相关的项目<ul>
<li>基于ID的传统做法：倾向选择以前交互过的物品。</li>
<li>问题：需要大量交互数据，那么就有<code>数据稀疏性</code>问题，且预测结果与ID具备强依赖关系（尤其在动态环境下），因此存在<code>冷启动</code>问题</li>
</ul>
</li>
</ul>
<h3 id="2-MRS小方向的问题"><a href="#2-MRS小方向的问题" class="headerlink" title="2. MRS小方向的问题"></a>2. MRS小方向的问题</h3><p>小方向：基于MRS充分利用辅助多模式信息补充历史交互信息，增强用户交互数据，更全面地描述用户偏好</p>
<p>需要提醒的是，当前多模态推荐正从”特征融合”向”语义对齐”演进.</p>
<p>具体到某个步骤，他们的做法：</p>
<p>早期想法（VBPR DeepStyle）：投影到低维构建，把特征通过级联&#x2F;相加的方式来与ID嵌入相结合，追溯到VBPR</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714101543088.png" alt="image-20250714101543088">	</p>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714101955634.png" alt="image-20250714101955634">	</p>
<h4 id="2-1-Feature-Extraction"><a href="#2-1-Feature-Extraction" class="headerlink" title="2.1 Feature Extraction"></a>2.1 Feature Extraction</h4><blockquote>
<p>主要是在视觉和文本模态中提取，从源头增强数据集的质量表示</p>
</blockquote>
<p>image</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/very-deep-convolutional-networks-for-large">VGG-2014</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/caffe-convolutional-architecture-for-fast">Caffe-2014</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/going-deeper-with-convolutions">Inception-2015</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition">ResNet-2016</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">VIT-2021</a></li>
</ul>
<p>text</p>
<ul>
<li>NN-based<ul>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1016/0306-4573(88)90021-0">TF-IDF</a></li>
<li><a target="_blank" rel="noopener" href="https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf">PV-DM(PV-DBOW)</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/glove-global-vectors-for-word-representation">Glove-2014</a></li>
</ul>
</li>
<li>model-based<ul>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/distributed-representations-of-words-and-1">WordToVec-2013</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/on-the-properties-of-neural-machine">GRU-2014</a></li>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1007/s42979-020-00427-1">Sentence2Vec-2017</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional">BERT-2019</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/sentence-bert-sentence-embeddings-using">Sentence-BERT-2019</a></li>
</ul>
</li>
</ul>
<h4 id="2-2-Encoder"><a href="#2-2-Encoder" class="headerlink" title="2.2 Encoder"></a>2.2 Encoder</h4><ul>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.5555/2981562.2981720">MF</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07153">GCN</a></li>
</ul>
<blockquote>
<p>主要目的是转换原始数据（例如，用户行为数据、物品属性等）转化为捕获输入数据核心特征的固定大小嵌入，目前有两种Encoder：MF-based、Graph-based</p>
<p><img src="/img/loading.gif" data-original="C:\Users\ljy\AppData\Roaming\Typora\typora-user-images\image-20250714114759111.png" alt="image-20250714114759111"></p>
</blockquote>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714121010770.png" alt="image-20250714121010770">	</p>
<h5 id="MF-based"><a href="#MF-based" class="headerlink" title="MF-based"></a>MF-based</h5><p>VBPR [29] 通过将预训练深度卷积神经网络提取的视觉特征融入矩阵分解框架，显著提升了偏好预测的准确性。</p>
<p>VMCF [30] 构建了融合商品外观特征与商品关联网络的产品亲和力图谱。</p>
<p>ACF [31] 创新性地引入商品级与组件级双重注意力机制，开创了注意力机制在协同过滤中的先河。</p>
<p>JRL [32] 及其升级版eJRL采用多视角机器学习技术整合多元信息源，无需重新训练即可实现精准的Top-N推荐。</p>
<p>DVBPR [33] 与UVCAN [36] 分别通过视觉信号融合与协同注意力机制优化推荐效果。</p>
<p>MAML [37] 和ADDVAE [48] 专注于通过神经网络建模文本与视觉特征的联合表征，其中ADDVAE还探索了解耦表征技术。</p>
<p>AMR [38] 运用对抗学习增强推荐系统的鲁棒性，有效抵御潜在攻击。IMRec通过聚焦本地新闻细节，使推荐流程更符合用户阅读习惯。</p>
<p>EliMRec [49] 与DMRL [55] 则分别采用因果推理与解耦表征技术，在降低系统偏差的同时，更精准地捕捉各模态的独立特征</p>
<h5 id="Graph-based"><a href="#Graph-based" class="headerlink" title="Graph-based"></a>Graph-based</h5><p>GraphCAR [34] 将多媒体内容与传统协同过滤方法相结合。</p>
<p>MMGCN [13] 采用图卷积网络学习各模态表征，再与ID嵌入融合形成最终物品表征。</p>
<p>MGAT [39] 在MMGCN框架基础上，使用标准GCN聚合方式及类似的结果融合方法。</p>
<p>GRCN [40] 通过识别并剪除噪声边来优化用户-物品交互图。</p>
<p>MKGAT [41] 采用基于实体的方法构建多模态知识图谱，为不同数据类型设计专用编码器，并运用注意力层实现邻域实体信息的有效聚合。</p>
<p>PMGT [43]是一个利用融合多模态特征与交互的预训练模型，通过注意力机制生成多模态嵌入表示。这些嵌入会进一步融合位置和角色编码，为预训练及下游任务初始化节点表征。</p>
<p>DualGNN[14]创新性地构建用户-用户关系图以挖掘潜在偏好模式</p>
<p>LATTICE[15]则构建物品-物品关系图来捕捉语义关联信号。</p>
<p>HHFAN[44]设计异质图整合用户、物品及多模态信息，采用随机游走实现分类型邻域采样，通过全连接层统一异构节点向量空间，并运用LSTM在类型内特征聚合网络中整合同类节点表征。</p>
<p>MVGAE[45]作为多模态变分图自编码器，利用模态专属变分编码器学习节点表示。</p>
<p>PAMD[46]采用解耦编码器分离对象共性与特性表征，通过对比学习实现跨模态对齐。</p>
<p>MMGCL[47]将自监督学习与图方法相结合应用于微视频推荐，其创新的负采样策略强化了模态间关联。</p>
<p>EgoGCN[50]提出高效的图融合方法，突破单模态图信息传播限制，聚合邻域节点的跨模态信息。</p>
<p>InvRL[51]通过跨环境稳定物品表征学习解决多媒体推荐中的伪相关性问题。</p>
<p>A2BM2GL[52]协同语义表征学习建模节点与视频特征，采用注意力反瓶颈模块增强关系表达能力，并设计自适应推荐损失函数动态响应用户偏好变化。</p>
<p>HCGCN[53]在LATTICE框架上引入协同聚类与物品聚类损失，优化用户-物品偏好反馈并调节模态重要性。</p>
<p>SLMRec[54]提出多模态推荐的自监督学习框架，通过节点自判别任务揭示物品隐含的多模态模式。</p>
<p>BM3[56]通过用随机丢弃策略替代负样本采样机制，简化了SLMRec框架。</p>
<p>MMSSL[57]设计了基于对抗性扰动的模态感知交互式结构学习范式，并提出跨模态对比学习方法以解耦模态间的共性与特性特征。</p>
<p>BCCL[58]整合了数据增强的偏置约束模块、模态感知模块与稀疏增强模块，协同生成高质量样本。</p>
<p>FREEDOM[59]通过冻结物品关系图并净化用户-物品交互图的噪声，优化了LATTICE模型。</p>
<p>MGCN[60]利用物品行为信息提纯模态特征以降低噪声干扰，并基于用户行为建模模态偏好。</p>
<p>PaInvRL[61]自适应平衡经验风险最小化与不变风险最小化损失，通过帕累托最优解实现模型性能提升。</p>
<p>DRAGON[62]通过构建同质与异质图学习用户与物品的双重表征。</p>
<p>LGMRec[63]将捕捉局部拓扑细节的局部嵌入与考虑超图依赖的全局嵌入相融合。</p>
<p>LLMRec[64]采用三种简洁高效的大语言模型图增强策略提升推荐性能。</p>
<p>PromptMM[65]通过解耦协同关系实现增强式知识蒸馏。</p>
<p>MCDRec[66]融合模态感知特征与协同信息优化物品表征，并采用扩散感知表示对用户-物品交互图降噪。</p>
<p>DiffMM[67]引入精心设计的模态感知图扩散模型以改进用户表征学习。</p>
<p>SOIL[68]从构建兴趣感知图的角度挖掘候选物品。</p>
<p>CKD[69]致力于解决模态失衡问题以充分利用所有模态信息。</p>
<p>MENTOR[75]在保持交互信息的同时通过多级跨模态对齐利用对齐模态。</p>
<p>GUME[70]结合MGCN与MENTOR在长尾场景中表现卓越。</p>
<p>POWERec[71]利用提示学习建模特定模态兴趣。</p>
<p>DGVAE[72]采用图卷积网络解构评分与多模态信息，从物品关系图学习表征并通过多模态数据文本投射增强可解释性。</p>
<p>VMoSE[73]基于不确定性估计自适应采样融合噪声多模态信号以增强鲁棒性。</p>
<p>SAND[74]无需负样本高效对齐模态通用表征，并清晰分离模态独有表征以保留独立信息。</p>
<h4 id="2-2-Multimodal-Fusion"><a href="#2-2-Multimodal-Fusion" class="headerlink" title="2.2 Multimodal Fusion"></a>2.2 Multimodal Fusion</h4><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714122139144.png" alt="image-20250714122139144">	</p>
<blockquote>
<p>融合的时机和融合的策略两个维度分析。</p>
<p>在策略上有两个维度分析：<code>Element-wise or Concatenation</code> and<code> Attentive or Heuristic</code></p>
<ul>
<li>第一个维度上，Concatenation的方法提供了更深入的集成，但无疑会<code>放大噪声</code></li>
<li>第二个维度上，注意力机制提供了更具适应性的处理，但是增加了计算成本</li>
<li>往往会结合融合的时机进行协同</li>
</ul>
<p>​	<img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714123001213.png" alt="image-20250714123001213"></p>
</blockquote>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714123251159.png" alt="image-20250714123251159">	</p>
<h5 id="Earyly-Fusion"><a href="#Earyly-Fusion" class="headerlink" title="Earyly Fusion"></a>Earyly Fusion</h5><blockquote>
<p>在编码前的模态融合，使得编码器学习到更丰富的表示，方法有级联、注意力机制和基于神经网络的集成方法，旨在创建多模式数据的统一表示，但可能会导致模式特定特征因过早集成而未得到充分利用。</p>
<p>$E &#x3D; Encoder(U, Aggr(Im))$</p>
</blockquote>
<p>VBPR[29]和GraphCAR[34]直接在矩阵分解框架中运用视觉特征。</p>
<p>VMCF[30]在此基础上提出视觉矩阵协同分解方法</p>
<p>DVBPR[33]则采用深度神经网络提升视觉数据表征能力。注意力机制被广泛应用于动态分配多模态权重</p>
<p>ACF[31]、VECF[35]、UVCAN[36]、MAML[37]、GRCN[40]、IMRec[42]、PMGT[43]、BCCL[58]和MCDRec[66]等研究通过注意力机制实现模态权重的精准分配。</p>
<p>MKGAT[41]构建了融合模态特征的多模态知识图谱以增强推荐系统。</p>
<p>最新研究LATTICE[15]利用原始特征构建单模态物品关系图，并通过注意力机制动态融合权重形成模态专属图谱。</p>
<p>HCGCN[53]则采用多模态物品图谱增强用户-物品交互图，在相似物品中发现用户偏好并实现同步聚类。</p>
<h5 id="Lately-Fusion"><a href="#Lately-Fusion" class="headerlink" title="Lately Fusion"></a>Lately Fusion</h5><blockquote>
<p>编码后将各个模式的编码器统一起来，充分利用每个模式的优势，在某些模态很丰富的时候可能会更有效，但可能会限制模型提取不同模式之间相关性的能力</p>
<p>$E ̄ &#x3D; Aggr(Encoder(U, Im))$</p>
</blockquote>
<p>JRL[32]、MMGCN[13]、DualGNN[14]、MMGCL[47]、EgoGCN[50]、FREEDOM[59]、MGCN[60]、DRAGON[62]、LGMRec[63]、DiffMM[67]、SOIL[68]和POWERec[71]等模型先独立处理各模态信息，再通过有效整合提升预测精度。</p>
<p>HHFAN[44]采用自注意力神经网络融合全模态信息，强化模型对关键特征的聚焦能力。</p>
<p>PAMD[46]、A2BM2GL[52]和LLMRec[64]运用注意力机制动态调节各模态对最终预测的影响，形成灵活的情境感知融合策略。</p>
<p>MVGAE[45]则通过专家乘积框架协调模态专属分布，确保模态输入的有效融合。部分方法专注于辅助模态对齐任务以实现更优整合，</p>
<p>ADDVAE[48]、MMSSL[57]、PromptMM[65]、GUME[70]、DGVAE[72]、SAND[74]、VMoSE[73]和MENTOR[75]均引入辅助任务促进模态融合。</p>
<p>EliMRec[49]从因果视角整合模态，CKD[69]采用平均处理效应(ATE)策略量化各模态对推荐结果的影响。</p>
<p>SMORE[76]创新性地将多模态特征映射至频域，在谱空间实现特征融合。</p>
<h3 id="3-损失函数选择"><a href="#3-损失函数选择" class="headerlink" title="3. 损失函数选择"></a>3. 损失函数选择</h3><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714123850469.png" alt="image-20250714123850469">		</p>
<h4 id="Supervised-Learning（主任务）"><a href="#Supervised-Learning（主任务）" class="headerlink" title="Supervised Learning（主任务）"></a>Supervised Learning（主任务）</h4><ul>
<li>Pointwise Loss评估单个预测的准确性<ul>
<li>均方差Cross-Entropy Loss： $\mathcal{L}<em>{MSE} &#x3D; \frac{1}{|\mathcal{D}|}\sum</em>{(u,i)\in\mathcal{D}}(r_{u,i} - y_{u,i})^2$</li>
<li>交叉熵Cross-Entropy Loss：$\mathcal{L}<em>{CE} &#x3D; -\frac{1}{|\mathcal{D}|}\sum</em>{(u,i)\in\mathcal{D}}[r_{u,i}\log(y_{u,i}) + (1-r_{u,i})\log(1-y_{u,i})]$</li>
</ul>
</li>
<li>Pairwise Loss表示item的排名好坏<ul>
<li>Bayesian Personalized Ranking：$\mathcal{L}<em>{BPR} &#x3D; \sum</em>{(u,i^+,i^-)\in\mathcal{D}} -\log\sigma(y_{u,i^+} - y_{u,i^-})$</li>
<li>Hinge Loss：$\mathcal{L}<em>{Hinge} &#x3D; \max(0, 1 - r</em>{u,i} \cdot y_{u,i})$</li>
</ul>
</li>
</ul>
<h4 id="SSL（辅助任务）"><a href="#SSL（辅助任务）" class="headerlink" title="SSL（辅助任务）"></a>SSL（辅助任务）</h4><blockquote>
<p>允许推荐系统有效地利用未标记的数据，提取有意义的表示并即使在数据稀疏的情况下也做出准确的预测</p>
</blockquote>
<ul>
<li><p>Feature-based SSL（创建预测任务 &#x2F; 重构某个特征，从而学习到更鲁棒的representations）</p>
<ul>
<li>$\omega &#x3D; C_{feature}(R, E, M) &#x3D; (R, \text{Perturb}(E), M)$</li>
<li>$\omega &#x3D; C_{structure}(R, E, M) &#x3D; (\text{Perturb}(R), E, M)$</li>
</ul>
</li>
<li><p>Structure-based SSL（在数据结构上操作，例如user-item关系中，使用图的方法通过节点相似性 &#x2F; 子图模式来生成监督信号，增强捕获复杂关系能力）</p>
</li>
</ul>
<p>相关损失函数：</p>
<ul>
<li>$\mathcal{L}_{InfoNCE} &#x3D; \mathbb{E}\left[-\log\frac{\exp(f(\omega’_i,\omega’’<em>i))}{\sum</em>{\forall i,j}\exp(f(\omega’_i,\omega’’_j))}\right]$</li>
<li><strong>Jensen-Shannon Divergence:</strong>  $\mathcal{L}_{JS} &#x3D; \mathbb{E}[-\log\sigma(f(\omega’_i,\omega’’_i))] - \mathbb{E}[\log(1-\sigma(f(\omega’_i,\omega’’_j)))]$</li>
</ul>
<h4 id="现有研究"><a href="#现有研究" class="headerlink" title="现有研究"></a>现有研究</h4><p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250714124630069.png" alt="image-20250714124630069">	</p>
<p>仅VMCF[30]、VECF[35]、UVCAN[36]、IMRec[42]、PMGT[43]、ADDVAE[48]、A2BM2GL[52]、BM3[56]、BCCL[58]、DGVAE[72]及VMoSE[73]等采用逐点损失函数。</p>
<p>Feature-based SSL：</p>
<ul>
<li>PAMD[46]利用自监督信号辅助特征层面的解耦表征学习</li>
<li>A2BM2GL[52]通过正则化与组合解注意力机制，实现评分数据与文本内容解耦特征的对齐</li>
<li>BCCL[58]创新性提出偏差约束数据增强方法，确保对比学习中增强样本的质量</li>
<li>MGCN[60]、LGMRec[63]、SOIL[68]、GUME[70]从特征维度运用对比学习提升表征质量</li>
<li>PromptMM[65]设计可学习提示模块，动态弥合教师模型多模态语境编码与学生模型协同关系建模的语义鸿沟</li>
<li>MCDRec[66]、DiffMM[67]、DGVAE[72]与VMoSE[73]通过优化KL散度使后验分布逼近先验分布</li>
<li>SAND[74]利用自监督信号区分模态专属与模态通用表征</li>
</ul>
<p>Structure-based SSL：</p>
<ul>
<li>MMGCL[47]采用模态掩码与模态边丢弃技术，从结构视角增强表征的模态一致性</li>
<li>A2BM2GL[52]通过注意力机制动态学习近程与远程邻接节点权重，获取更具表现力的表征</li>
<li>HCGCN[53]从拓扑结构角度运用对比损失协调多模态特征</li>
<li>SLMRec[54]融合多种结构视角的对比学习提升表征质量</li>
</ul>
<p>混合自监督学习：</p>
<ul>
<li>BM3[56]在特征与结构双层面融合多种自监督信号</li>
<li>MMSSL[57]与MENTOR[75]在特征层面实现多模态表征对齐，并采用特征视角的对比学习增强表征能力</li>
</ul>
<h2 id="二、讲故事技巧"><a href="#二、讲故事技巧" class="headerlink" title="二、讲故事技巧"></a>二、讲故事技巧</h2><h3 id="问题的描述"><a href="#问题的描述" class="headerlink" title="问题的描述"></a>问题的描述</h3><p>用图来描述融合后噪声放大问题的严重性以及融合前噪声过滤的必要性（前人没有描述的问题编的故事显得更创新）：</p>
<ul>
<li>文字中 hammer是噪声，与主题不相关，导致融合的时候偏向于hammer，增加了不相关物品的similarity，放大了噪声</li>
<li>图像中，如果图像有一个blur的noise，导致融合的时候减小了相关物品的similarity，也是放大了噪声</li>
</ul>
<p><img src="/img/loading.gif" data-original="https://yuezi-1308313119.cos.ap-guangzhou.myqcloud.com/typora-user-images/image-20250627114559650.png" alt="image-20250627114559650">	</p>
<p>用柱状图+折线图来描述模态缺失的问题</p>
<h2 id="三、PROBLEM-DEFINITION-AND-NOTATIONS"><a href="#三、PROBLEM-DEFINITION-AND-NOTATIONS" class="headerlink" title="三、PROBLEM DEFINITION AND NOTATIONS"></a>三、PROBLEM DEFINITION AND NOTATIONS</h2><h3 id="Preliminary-1：历史行为数据与图结构"><a href="#Preliminary-1：历史行为数据与图结构" class="headerlink" title="Preliminary 1：历史行为数据与图结构"></a>Preliminary 1：历史行为数据与图结构</h3><blockquote>
<p>Basic Notation</p>
</blockquote>
<p>Let:</p>
<ul>
<li>$\mathcal{U} \in \mathbb{R}^{|\mathcal{U}|\times d}$: Set of users</li>
<li>$\mathcal{I} \in \mathbb{R}^{|\mathcal{I}|\times d}$: Set of items<br>where:<ul>
<li>$d$: Hidden dimension</li>
<li>$|\mathcal{U}|, |\mathcal{I}|$: Number of users&#x2F;items</li>
</ul>
</li>
</ul>
<blockquote>
<p>Interaction Matrix</p>
</blockquote>
<ul>
<li>$R &#x3D; [r_{u,i}]_{|\mathcal{U}|\times|\mathcal{I}|}$: User-item interaction matrix</li>
</ul>
<blockquote>
<p>Graph-based Encoder Definition</p>
</blockquote>
<p>For a graph $\mathcal{G} &#x3D; (\mathcal{V}, \mathcal{E})$:</p>
<ul>
<li>$\mathcal{V}$: Node set ($|\mathcal{V}| &#x3D; |\mathcal{U}| + |\mathcal{I}|$)</li>
<li>$\mathcal{E}$: Edge set</li>
<li>$E &#x3D; {\mathcal{U}|\mathcal{I}}$: Embeddings for nodes $\mathcal{V}$</li>
</ul>
<h3 id="Preliminary-2：多模态任务"><a href="#Preliminary-2：多模态任务" class="headerlink" title="Preliminary 2：多模态任务"></a>Preliminary 2：多模态任务</h3><p>For multimodal scenario:</p>
<ul>
<li><p>$\mathcal{I}_m \in \mathbb{R}^{|\mathcal{I}|\times d_m}$: Item sets per modality<br>where:</p>
<ul>
<li>$m \in \mathcal{M}$: Set of modalities</li>
<li>$d_m$: Modality-specific hidden dimension</li>
</ul>
</li>
<li><p>$\mathcal{G}_m &#x3D; (\mathcal{V}_m, \mathcal{E})$: Modality-specific graphs<br>where:</p>
<ul>
<li>$|\mathcal{V}_m| &#x3D; |\mathcal{U}| + |\mathcal{I}_m|$</li>
<li>$E_m &#x3D; {\mathcal{U}|\mathcal{I}_m}$: Modality-specific embeddings</li>
</ul>
</li>
</ul>
<blockquote>
<p>Matrix Factorization Formulation</p>
</blockquote>
<p>The core recommendation task approximates:<br>$$<br>R \approx \hat{R} &#x3D; \mathcal{U}\mathcal{I}^T<br>$$</p>
<p>With loss function:<br>$$<br>\min_{\mathcal{U},\mathcal{I}} |R - \hat{R}|_F^2 + \lambda(|E|_F^2)<br>$$</p>
<blockquote>
<p>Graph Convolution Formulation</p>
</blockquote>
<p>For GCN layers:<br>$$<br>E^{(l)} &#x3D; \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}E^{(l-1)}W^{(l-1)})<br>$$<br>where:</p>
<ul>
<li>$A$: graph adjacency matrix</li>
<li>$\tilde{A} &#x3D; A + I$ (with self-loops)</li>
<li>$\tilde{D}$: Degree matrix of $\tilde{A}$</li>
<li>$W^{(l-1)}$: Trainable weights</li>
<li>$\sigma$: Activation function</li>
</ul>
<h3 id="多模态推荐任务目标"><a href="#多模态推荐任务目标" class="headerlink" title="多模态推荐任务目标"></a>多模态推荐任务目标</h3><ul>
<li>多模态推荐的目标是通过基于预测分数$ \hat{r}_{u,i}$ 对用户的所有物品进行排序，来准确预测用户与物品之间的交互可能性</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>Amozon  dataSets</p>
<ul>
<li>会存在一些损坏的信息：可通过预训练模型如BERT&#x2F;Sentece-Transformer增强文本分析</li>
</ul>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul>
<li>CIKM’24 | 小红书AlignRec:多模态推荐的对齐和训练框架<ul>
<li><strong>挑战1</strong>: 如何对齐多模态表征。包括内容模态之间(如图文)的对齐, 以及内容模态与ID模态之间的对齐。此外, 论文所提方法是做召回, 因此, 还需要特别关注User与Item表征的对齐。为此, 作者所提出的AlignRec框架, 主要做了3个方面的对齐:<ul>
<li>内容模态内的对齐(inter-content alignment, 简称ICA), 主要指视觉模态和文本模态</li>
<li>内容模态与ID模态的对齐(content-category alignment, 简称CCA)</li>
<li>User和Item的对齐(user-item alignment, 简称UIA), 主要通过<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=248562312&content_type=Article&match_order=1&q=Cosine%E7%9B%B8%E4%BC%BC%E5%BA%A6&zhida_source=entity">Cosine相似度</a>, 在召回环节需要基于用户向量去做向量检索</li>
</ul>
</li>
<li><strong>挑战2</strong>: 如何平衡好内容模态和ID模态之间的学习速度问题。内容模态可能需要超大规模的数据和超长时间去训练, 而ID模态的学习更新可能只需要几个epoch。这里作者的解决方案使用mLLMs(多模态大语言模型), 在预训练阶段就提前做好内容模态内的对齐。</li>
<li><strong>挑战3</strong>: 如何评估多模态特征对推荐系统的影响。引入不合适的多模态信息可能需要更多的精力去做纠正, 甚至可能影响推荐系统的性能。为此, 作者也提出了多个中间评估指标。</li>
</ul>
</li>
<li>RecSys’24 | 快手M3CSR:基于多模态的短视频冷启动推荐<ul>
<li><strong>表征空间不一致问题:</strong> 推荐系统的ID Embed是用户行为(协同过滤)驱动的, 而一般的多模态表征(如使用Bert的文本表征)是基于内容本身来学习的, 它们的表征空间是不一致的, 是有Gap的。而这也是业界大家一般不会将ID Embed与多模态原始表征直接Concat的主要原因。</li>
<li><strong>用户对多模态的偏好不一:</strong> 这个也很好理解, 就比如有些人喜欢一首歌曲是因为歌曲动听的旋律, 而有些人喜欢一首歌曲可能是因为歌曲的歌词让人共情。而快手这里则提出要显式建模用户对内容的不同模态(文本、视觉和音频)的不同偏好。</li>
</ul>
</li>
<li>快手QARM:量化对齐多模态推荐<ul>
<li><strong>表征不匹配问题:</strong> 预训练的多模态模型是以NLP&#x2F;CV任务监督学习, 如图像文本匹配的自监督任务, 而推荐模型是ID特征主导的, 由真实的User-Item交互数据来驱动学习, 这就导致两类任务目标相对独立, 缺乏一致的目标。</li>
<li><strong>表征不更新问题:</strong> 生成的多模态表征是静态的, 总是存储在缓存中, 作为推荐模型的额外固定输入, 无法通过推荐模型梯度更新, 这对下游任务训练更加不友好。<ul>
<li><em>美食视频的视觉特征可能被预训练模型识别为“颜色鲜艳物体”，但推荐系统更需捕捉“引发用户食欲”的潜在信号。</em></li>
</ul>
</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671706753">https://zhuanlan.zhihu.com/p/671706753</a></li>
<li>SIGIR2025 | LightCCF: 不用GCN, 同样可以利用对比学习在协同过滤中实现邻域聚合能力</li>
<li>WSDM2025最佳论文: 从谱视角揭开推荐系统流行度偏差放大之谜</li>
<li>WSDM2025 | 基于谱域扩散模型的协同过滤</li>
<li>OneRec: 快手生成式推荐结合Scaling Law+DPO实现端到端推荐</li>
</ul>
<h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.15711v1">A Survey on Multimodal Recommender Systems:  Recent Advances and Future Directions</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Jinfeng-Xu/Awesome-Multimodal-Recommender-Systems">Awesome-Multimodal-Recommender-Systems</a></li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yuezi2048.github.io">yuezi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yuezi2048.github.io/2025/06/27/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%A2%B3%E7%90%86%E5%92%8Cidea/">https://yuezi2048.github.io/2025/06/27/2.Areas🌐/science/小论文准备/论文复现总结/工作的梳理和idea/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://yuezi2048.github.io" target="_blank">yuezi</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/">论文复现总结</a></div><div class="post-share"><div class="social-share" data-image="/img/touxiang.jpg" data-sites="qq,wechat"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/06/25/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E6%B0%B4%E8%AE%BA%E6%96%87%E7%9A%84%E7%A8%8B%E5%BA%8F%E7%8C%BF/%E5%B0%8F%E8%AE%BA%E6%96%87%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83/" title="小论文从入门到放弃"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">小论文从入门到放弃</div></div><div class="info-2"><div class="info-item-1">看哪些论文不用看的：普刊、中文核心、ei会议、ei源刊（sci四区）  （发ei会议可以参考一下）  要看的：SCI（SSCI）四个级别，CCF会议（三个级别）  二三区（CCFB）没必要看，不上不下 看最好的（一区，CCFA）和最烂的（四区，CCFC） 一区（顶刊）：关注吹牛皮的方法 四区：赤裸裸A+b+c 的看的很清楚，明确学术裁缝的方法    读论文的方法 不用精读，挑重点  摘要引言（抄）+ 相关工作（抄） + 方法概括（重点看） + 实验效果  要点：按部分（标点符号，转折词等断句）来读英文，对我们有用的再精读  	 方法：CLIP +...</div></div></div></a><a class="pagination-related" href="/2025/06/27/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/SMORE2(%E5%9F%BA%E4%BA%8E%E9%A2%91%E8%B0%B1%E8%9E%8D%E5%90%88+%E8%A1%8C%E4%B8%BA%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88)/" title="SMORE"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">SMORE</div></div><div class="info-2"><div class="info-item-1">多模态推荐系统研究总结当前领域存在的问题现有的多模态推荐系统在融合不同模态(如图片、文字等)时存在一个主要问题：直接融合会导致各模态特有的噪声被放大。例如，商品图片可能模糊不清，文字描述可能与商品不相关，这些噪声会影响推荐系统的准确性。 现有做法目前主要有以下几种方法：  简单融合方法：如拼接(concatenation)、按元素相加(element-wise sum)或注意力机制(attention mechanisms) 图神经网络方法：构建不同模态的图结构，然后进行融合 行为特征注入：将用户行为特征与模态特征结合来减少噪声  这些方法虽然取得了一定效果，但都存在直接融合模态导致噪声放大的问题。 本文提出的方法(SMORE)作者提出了SMORE模型，包含三个关键部分： 1. 基于频谱的模态表示融合(Spectrum Modality Fusion) 将模态特征通过傅里叶变换转换到频域 在频域设计动态滤波器来自适应地衰减噪声 通过点积操作融合不同模态 最后通过逆傅里叶变换回到原始空间  2. 多模态图学习(Multi-modal Graph...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/24/2.Areas%F0%9F%8C%90/science/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/temp/" title="temp"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-24</div><div class="info-item-2">temp</div></div><div class="info-2"><div class="info-item-1">这篇论文《Contrastive Modality-Disentangled Learning for Multimodal Recommendation》提出了一种创新的多模态推荐系统框架CMDL，通过解耦模态共享知识和模态特有知识来提升推荐性能。以下是对其解耦方法的详细分析： 1. 模态解耦的核心机制论文通过三个关键步骤实现模态解耦： (1) 初始表示生成 构建视觉和文本模态的item-item语义图（G_v和G_t） 使用LightGCN生成初始模态表示： 视觉表示 h_v^r 文本表示 h_t^r    (2) 表示解耦采用自注意力机制计算各模态的重要性权重： $a_v^r &#x3D; softmax(W_2σ(W_1h_v^r + b_1))a_t^r &#x3D; softmax(W_2σ(W_1h_t^r + b_1))$ 然后生成：  模态不变表示（共享知识）： $e_m^r &#x3D; a_v^r h_v^r + a_t^r h_t^r$  模态特定表示（特有知识）： e_v^r = h_v^r - e_m^re_t^r = h_t^r -...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">yuezi</div><div class="author-info-description">积微者速成</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">785</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">66</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">91</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yuezi2048"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://githubfast.com/yuezi2048" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_46345703" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="https://qm.qq.com/cgi-bin/qm/qr?k=dD62GIPTf5-iS4UdSfJRy7NHwsrCh3-j" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:joyLing@stumail.nwu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">积微者速成</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8E%A8%E8%8D%90%E7%9B%AE%E5%89%8D%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">一、多模态推荐目前的工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-RS%E5%A4%A7%E6%96%B9%E5%90%91%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">1. RS大方向的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-MRS%E5%B0%8F%E6%96%B9%E5%90%91%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">2. MRS小方向的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Feature-Extraction"><span class="toc-text">2.1 Feature Extraction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Encoder"><span class="toc-text">2.2 Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MF-based"><span class="toc-text">MF-based</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Graph-based"><span class="toc-text">Graph-based</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Multimodal-Fusion"><span class="toc-text">2.2 Multimodal Fusion</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Earyly-Fusion"><span class="toc-text">Earyly Fusion</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Lately-Fusion"><span class="toc-text">Lately Fusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E9%80%89%E6%8B%A9"><span class="toc-text">3. 损失函数选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Supervised-Learning%EF%BC%88%E4%B8%BB%E4%BB%BB%E5%8A%A1%EF%BC%89"><span class="toc-text">Supervised Learning（主任务）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SSL%EF%BC%88%E8%BE%85%E5%8A%A9%E4%BB%BB%E5%8A%A1%EF%BC%89"><span class="toc-text">SSL（辅助任务）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E7%A0%94%E7%A9%B6"><span class="toc-text">现有研究</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%AE%B2%E6%95%85%E4%BA%8B%E6%8A%80%E5%B7%A7"><span class="toc-text">二、讲故事技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E7%9A%84%E6%8F%8F%E8%BF%B0"><span class="toc-text">问题的描述</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81PROBLEM-DEFINITION-AND-NOTATIONS"><span class="toc-text">三、PROBLEM DEFINITION AND NOTATIONS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Preliminary-1%EF%BC%9A%E5%8E%86%E5%8F%B2%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%9B%BE%E7%BB%93%E6%9E%84"><span class="toc-text">Preliminary 1：历史行为数据与图结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Preliminary-2%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%BB%E5%8A%A1"><span class="toc-text">Preliminary 2：多模态任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8E%A8%E8%8D%90%E4%BB%BB%E5%8A%A1%E7%9B%AE%E6%A0%87"><span class="toc-text">多模态推荐任务目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TODO"><span class="toc-text">TODO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Refer"><span class="toc-text">Refer</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/22/Excalidraw/Drawing%202024-11-01%2015.37.37.excalidraw/" title="Drawing 2024-11-01 15.37.37.excalidraw">Drawing 2024-11-01 15.37.37.excalidraw</a><time datetime="2026-02-22T05:56:41.839Z" title="发表于 2026-02-22 13:56:41">2026-02-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/22/2.Areas%F0%9F%8C%90/01.project/condefather/yupao/back-end/7.%E4%BC%98%E5%8C%96%E4%B8%BB%E9%A1%B5%E6%80%A7%E8%83%BD-%E4%BC%98%E5%8C%96%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE/" title="7.优化主页性能-优化批量导入数据">7.优化主页性能-优化批量导入数据</a><time datetime="2026-02-22T05:56:41.257Z" title="发表于 2026-02-22 13:56:41">2026-02-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/31/2.Areas%F0%9F%8C%90/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/5.%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8/" title="5.工具调用">5.工具调用</a><time datetime="2026-01-31T03:54:22.000Z" title="发表于 2026-01-31 11:54:22">2026-01-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/30/2.Areas%F0%9F%8C%90/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/4.RAG%E8%BF%9B%E9%98%B6/" title="4.RAG进阶">4.RAG进阶</a><time datetime="2026-01-30T13:23:42.000Z" title="发表于 2026-01-30 21:23:42">2026-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/27/2.Areas%F0%9F%8C%90/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/3.RAG/" title="3.RAG">3.RAG</a><time datetime="2026-01-27T09:06:56.000Z" title="发表于 2026-01-27 17:06:56">2026-01-27</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/default.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2026 By yuezi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>