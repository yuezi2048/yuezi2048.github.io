<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>yuezi</title><meta name="author" content="yuezi"><meta name="copyright" content="yuezi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="积微者速成">
<meta property="og:type" content="website">
<meta property="og:title" content="yuezi">
<meta property="og:url" content="https://yuezi2048.github.io/page/24/index.html">
<meta property="og:site_name" content="yuezi">
<meta property="og:description" content="积微者速成">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yuezi2048.github.io/img/touxiang.jpg">
<meta property="article:author" content="yuezi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuezi2048.github.io/img/touxiang.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yuezi2048.github.io/page/24/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?bfb48b678d82bea9f9cc9d59227767e4";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"FPH6F2TOJL","apiKey":"06d9ea26c803ba912b4d8e13404ed34d","indexName":"blog","hitsPerPage":6,"languages":{"input_placeholder":"搜索","hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'yuezi',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'home'
}</script><link rel="stylesheet" href="/css/backgound.css"><meta name="generator" content="Hexo 8.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-original="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">603</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">85</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url(/img/backgroud.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">yuezi</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="site-info"><h1 id="site-title">yuezi</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://githubfast.com/yuezi2048" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_46345703" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="https://qm.qq.com/cgi-bin/qm/qr?k=dD62GIPTf5-iS4UdSfJRy7NHwsrCh3-j" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:joyLing@stumail.nwu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/07/4.Archives%F0%9F%97%91/KOB/10.Bot%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C/" title="10.Bot代码执行">10.Bot代码执行</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-03-07T11:04:48.000Z" title="发表于 2024-03-07 19:04:48">2024-03-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">用docker在云端进行代码部署 设置内存上线，可以支持其他的语言。 这里用的是joor来执行java代码（BotZone里有很多参考代码）。 注：我们的整体项目逻辑是一个闭环 		 既然要实现bot代码的实现，在前面的基础上，我们需要实现bot主键的一个传递，这涉及到了各个组件的通信问题。 接下来我们逐个击破。 	 一、初始化bot微服务1.1 创建微服务入口我们首先创建一个后端BotRunningSystem。 	 可以直接沿用MatchingSystem的依赖，然后注意导入我们的joor包来编译执行我们的java代码（后续可以替换成docker执行）。 这里微服务的主要逻辑在于和next step这里进行通信的。 	 然后建立后端后，我们要创建一个入口BotRunningApplication，  package com.zjxu.kob.botrunningsystem; import org.springframework.boot.SpringApplication; import...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/03/4.Archives%F0%9F%97%91/KOB/7.%E5%BE%AE%E6%9C%8D%E5%8A%A1%EF%BC%9A%E5%8C%B9%E9%85%8D%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%8A%EF%BC%89/" title="7.微服务：匹配系统（上）">7.微服务：匹配系统（上）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-03-03T02:44:40.000Z" title="发表于 2024-03-03 10:44:40">2024-03-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">00:0022:13 规划介绍22:1353:00 jwt验证53:00~1:07:00 前端匹配界面(只有前端)1:07:00 ~ 1:25:00 前端匹配界面(后端交互)1:25:00 ~ 1:44:00 生成同步地图(前端生成地图改成后端生成发送到前端)1:44:00 ~ 结束 聊天 一、匹配流程分析我们匹配的过程是一个异步的过程，得到结果后立即返回给前端 http：一问一答，而我们实现的是发送一个请求，可能返回多个结果或是要等待很长时间。 我们使用的是websocket请求：全双工通信方式 	 我们首先实现客户端和服务端，匹配系统（基于SpringCloud 微服务）在后面实现。 我们发现生成地图是在本地生成的，而我们由于是匹配对战，需要将地图放在云端服务器上  不止是生成地图，包括蛇的移动，蛇的判定，全部都要统一在后端，前端不要任何逻辑 只返回图像  	 二、配置websocket实现前后端通信websocket过程：websocket是一个类，收到来自前端的请求，就会开一个线程 创建一个类...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/28/4.Archives%F0%9F%97%91/KOB/5.%E9%85%8D%E7%BD%AEMysql%E4%B8%8E%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E6%A8%A1%E5%9D%97%EF%BC%88%E4%B8%8B%EF%BC%89/" title="5.配置Mysql与注册登录模块（下）">5.配置Mysql与注册登录模块（下）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-28T11:52:21.000Z" title="发表于 2024-02-28 19:52:21">2024-02-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">本次的主要任务：  将数据库中的id域变为自增 在pojo.User类中添加注解：@TableId(type &#x3D; IdType.AUTO)   实现&#x2F;user&#x2F;account&#x2F;token&#x2F;：验证用户名密码，验证成功后返回jwt token（令牌） 实现&#x2F;user&#x2F;account&#x2F;info&#x2F;：根据令牌返回用户信息 实现&#x2F;user&#x2F;account&#x2F;register&#x2F;：注册账号 实现前端的任务  一、前置工作1.1...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/25/4.Archives%F0%9F%97%91/KOB/4.%E9%85%8D%E7%BD%AEMysql%E4%B8%8E%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E6%A8%A1%E5%9D%97%EF%BC%88%E4%B8%8A%EF%BC%89/" title="4.配置Mysql与注册登录模块（上）">4.配置Mysql与注册登录模块（上）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-25T03:27:07.000Z" title="发表于 2024-02-25 11:27:07">2024-02-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">一、前置配置工作1.1 项目逻辑回顾 每一个前端Client都会与后端SpringBoot交互（传递参数），而在后端的过程中，我们需要进行查询mysql来得到我们的数据 数据库里的表本质上是可以看成是一个数组，而对数据库的操作我们可以看成算法里的字符串处理  	    1.2 mysql搭建和java连接数据库安装mysql略 基本操作回顾 关闭：net stop mysql80启动：net start mysql80连接用数据库服务：mysql -uroot -p123456    show databases;：列出所有数据库create database kob;：创建数据库drop database kob;：删除数据库use kob;：使用数据库kobshow tables;：列出当前数据库的所有表create table user(id int, username varchar(100))：创建名称为user的表，表中包含id和username两个属性。drop table user;：删除表insert into user values(1,...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/24/2.Areas%F0%9F%8C%90/03.algorithm/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952/" title="基础算法2">基础算法2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-24T08:49:42.000Z" title="发表于 2024-02-24 16:49:42">2024-02-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/03-algorithm/">03.algorithm</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/03-algorithm/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/">算法基础课</a></span></div><div class="content">一、高精度算法	  高精度具体可以做到1e6位数的加减以及对小于1w的数乘除 注：本质是用数组操作，因为我们运算一般是从低位开始算的，所以加减乘一般都是要倒序存，即下标从低位开始存。 1.1 高精度加法不难发现每次运算都会有进位0或者1  	    算法思路  倒序存储到数组A B 同时遍历两个数组，t &#x3D; t+A[i]+B[i]    t % 10加到后面，进位t 过更新为 t &#x2F; 10 最后的进位如果是1 再加进去 倒序将数组输出C就是答案  	    #include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;const int N = 1e6 + 10;// C = A + Bvector&lt;int&gt; add(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123;    vector&lt;int&gt; C;        int t = 0;  // 表示是否有进位    for (int i = 0;...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/17/4.Archives%F0%9F%97%91/KOB/2.%E8%8F%9C%E5%8D%95%E5%92%8C%E6%B8%B8%E6%88%8F%E7%95%8C%E9%9D%A2%E8%AE%BE%E8%AE%A1/" title="2.菜单和游戏界面设计">2.菜单和游戏界面设计</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-17T05:56:02.000Z" title="发表于 2024-02-17 13:56:02">2024-02-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">一、导航栏界面与跳转预期目标：我们要实现如下的页面以及响应的URL跳转（304），并且要求实现访问该界面时，对应的模块高亮。  页面我们一般放到view里面，我们每个模块建一个文件夹 	 1.1 公共子组件ContentField每个Index界面的公共子组件ContentField.vue，这里有三个点  这里用到了bootstrap的card组件框起来，我们可以如下简写  	  我们待填充的部分使用的是slot标签 为了美化，我们通过content-field加上了一个上边距  &lt;template&gt;  &lt;div class=&quot;container content-field&quot;&gt;    &lt;div class=&quot;card&quot;&gt;        &lt;div class=&quot;card-body&quot;&gt;            &lt;slot&gt;&lt;/slot&gt;        &lt;/div&gt;    &lt;/div&gt; ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/17/4.Archives%F0%9F%97%91/KOB/1.%E9%85%8D%E7%BD%AEgit%E7%8E%AF%E5%A2%83%E5%92%8C%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/" title="1.配置git环境和项目搭建">1.配置git环境和项目搭建</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-17T05:29:01.000Z" title="发表于 2024-02-17 13:29:01">2024-02-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">一、游戏系统设计框架我们的框架树里面，每个孩子都是对应一个导航栏（链接）  	  前后端分离的优越性：一个后端可以对应多个前端。 二、git基础应用为什么用git？  代码存档(commit)，可以回滚出历史代码 同步不同设备  怎么用git？  首先用git bash在home目录下输入ssh-keygen 生成密钥，找到公钥，放入平台的SSH密钥  	  创建项目文件KOB，使用git init 生成.git隐藏文件夹，再新建一个readme.md 然后我们如果想commit 流程如下 ps：我们应该要去掉–global，以此来保证每个项目不用这个信息    	 	  我们后续提交代码就这固定的操作就OK  	  接收方面， 我们可以使用git pull拉取最新的代码  三、后端创建3.1...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/12/3.Resources%F0%9F%93%94/communication/note/" title="note">note</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-12T04:51:08.000Z" title="发表于 2024-02-12 12:51:08">2024-02-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/3-Resources%F0%9F%93%94/">3.Resources📔</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/3-Resources%F0%9F%93%94/communication/">communication</a></span></div><div class="content">情商：通过掌控自己的情绪，从而使智商得到最大的利用率。 由于对自我感官敏感度的提高：  感受到他人的情绪，理解对方 影响他人的情绪，影响对方的行为  </div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/05/2.Areas%F0%9F%8C%90/front-end/vue/Vue%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/" title="Vue学习笔记2">Vue学习笔记2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-05T15:26:40.000Z" title="发表于 2024-02-05 23:26:40">2024-02-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/front-end/">front-end</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/front-end/vue/">vue</a></span></div><div class="content">Vue学习笔记流（下）1. 好友列表	  UserListView &lt;template&gt;  &lt;contentBase&gt;    &lt;!-- 函数调用传参直接加括号 不加括号是执行对象 在react中要封装成一个匿名函数 --&gt;    &lt;div class=&quot;card&quot; v-for=&quot;user in users&quot; :key=&quot;user.id&quot; @click=&#x27;open_user_profile(user.id)&#x27;&gt;      &lt;div class=&quot;card-body&quot;&gt;        &lt;div class=&quot;row&quot;&gt;          &lt;div class=&quot;col-1 img-field&quot;&gt;            &lt;img class=&quot;img-fluid&quot; :src=&quot;user.photo&quot;...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/05/2.Areas%F0%9F%8C%90/front-end/vue/Vue%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="Vue学习笔记">Vue学习笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-05T15:26:35.000Z" title="发表于 2024-02-05 23:26:35">2024-02-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/front-end/">front-end</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/front-end/vue/">vue</a></span></div><div class="content">Vue学习笔记流（上）1. 创建项目 安装Node.js  xxx安装@vue&#x2F;cli（npm i -g @vue&#x2F;cli）  vue，启动！– 使用vue ui命令   	    在 输出的命令行内，我们可以进行访问前端界面了  2. vue项目前置基本概念java html js css基础略。 		 大佬们常说的三剑客框架对应vue文件里的三个标签 （分别对应 html &amp; js &amp; CSS）， &lt;template&gt;&lt;/template&gt;&lt;script&gt;&lt;/script&gt;&lt;style scoped&gt;&lt;/style&gt;  vue重要优势是通过使用scoped，不同CSS样式之间互不影响，提供了组件化框架，原理是使用了随机值区分、 	 2.1 vue重要组件属性速查script部分export...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/05/4.Archives%F0%9F%97%91/KOB/3.%E8%9B%87%E7%9A%84%E8%AE%BE%E8%AE%A1/" title="3.蛇的设计">3.蛇的设计</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-05T15:26:18.000Z" title="发表于 2024-02-05 23:26:18">2024-02-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/">4.Archives🗑</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/4-Archives%F0%9F%97%91/KOB/">KOB</a></span></div><div class="content">一、前置bug解决假设有一种情况，他们下一步会进入到同一个格子，这就会对优势方不利（有路可走），造成游戏不公平的情况 	 我们这里使用了头节点同一时刻不能走到同一个格子的策略 这里给出的方式是调整地图的长宽为13*14，这样一来，每回合蛇的坐标永远不会重合。 	    那么随之会引出另外一个问题：地图不是轴对称了 我们调整为中心对称，注意canvas坐标轴的定义 	    请注意，我们生成地图应当最终是在后端生成，否则会导致玩家可以修改前端代码，目前写在前端只是方便调试。 	    二、蛇的设计2.1 初始创建蛇头初始的时候是一个点，我们规定前10回合，每回合蛇长度加一，后面每三步变长一个 我们的蛇实质上是一堆圆圈的序列，我们先定义一个这个圈圈类cell.js 我们以圆心为单位，根据canvas坐标系我们可以得知x 和y坐标和canvas的对应关系 	  export class cell &#123;    constructor(r, c) &#123;        this.r = r;        this.c = c;        this.x = c +...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/04/0.InBox%F0%9F%90%AD/md/link/" title="link">link</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-04T08:00:32.000Z" title="发表于 2024-02-04 16:00:32">2024-02-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/0-InBox%F0%9F%90%AD/">0.InBox🐭</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/0-InBox%F0%9F%90%AD/md/">md</a></span></div><div class="content">test layerthis is TEST CONTEST.^e6e080 </div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/04/6.Excalidraw%F0%9F%8E%A8/Drawing%202024-02-04%2012.05.15.excalidraw/" title="Drawing 2024-02-04 12.05.15.excalidraw">Drawing 2024-02-04 12.05.15.excalidraw</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-04T07:23:15.000Z" title="发表于 2024-02-04 15:23:15">2024-02-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/6-Excalidraw%F0%9F%8E%A8/">6.Excalidraw🎨</a></span></div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/03/0.InBox%F0%9F%90%AD/md/Demo/" title="Demo">Demo</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-02-03T16:00:00.000Z" title="发表于 2024-02-04 00:00:00">2024-02-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/0-InBox%F0%9F%90%AD/">0.InBox🐭</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/0-InBox%F0%9F%90%AD/md/">md</a></span></div><div class="content">This is just a demo. tipsctrl+g 全局图谱  ctrl+j 局部图谱 You can directly access the  [[link]], or the Importer!. And you can access specific  [[link#^1b991c|link]]. even you can see this by add ! .![[link#^e6e080]]  this is Reference  public static void main(String args[]) &#123;	System.out.println(&quot;You can input your codes like this.&quot;);&#125;    this is the first point   1.1 this is the subpoint. 1.2 this is the secone subpoint.    this is second point   2.1 this is the subpoint too.   ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/00%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E5%85%A8%E6%96%87%2024854%20%E4%B8%AA%E8%AF%8D%EF%BC%89/" title="00 预训练语言模型的前世今生（全文 24854 个词）">00 预训练语言模型的前世今生（全文 24854 个词）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  预训练语言模型的前世今生 - 从Word Embedding到BERT  本篇文章共 25027 个词，一个字一个字手码的不容易，转载请标明出处：预训练语言模型的前世今生 - 从Word Embedding到BERT - 二十三岁的有德 [TOC] Bert 最近很火，应该是最近最火爆的 AI 进展，网上的评价很高，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多 NLP 的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是 Bert 具备广泛的通用性，就是说绝大部分 NLP...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/04%20%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88n%E5%85%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%89/" title="04 统计语言模型（n元语言模型）">04 统计语言模型（n元语言模型）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  预训练预先训练 我们有两个相似的任务 A 和 B，任务 A 已经完成了得到了一个模型 A 任务 B（数据量小） 用到了一个特性：CNN 浅层参数通用 任务 B 就可以使用模型 A 的浅层参数，后面的参数通过任务 B 训练–》1. 冻结（浅层参数不变）2. 微调（变） 任务 B（大数据）可以训练出模型 B（我还可以使用模型 A...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/07%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%94%B9%E9%80%A0%E7%AE%80%E4%BB%8B%EF%BC%88%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%90%91%E9%87%8F%EF%BC%89/" title="07 预训练语言模型的下游任务改造简介（如何使用词向量）">07 预训练语言模型的下游任务改造简介（如何使用词向量）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  Word2Vec –》 是一个神经网络语言模型，其次他的主要任务是做（生成词向量，Q）![image-20220614194418918](..&#x2F;..&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220614194418918.png) Word2Vec 模型是不是预训练模型？（是） 一定是 什么是预训练？ 给出两个任务 A 和 B，任务 A 已经做出了模型 A，任务 B 无法解决（通过使用模型 A，加快任务的解决） 给你一个 NLP...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/09%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention%20%EF%BC%89/" title="09 什么是注意力机制（Attention ）">09 什么是注意力机制（Attention ）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  Attention（注意力机制）你会注意什么？ 大数据（什么数据都有，重要的，不重要的） 对于重要的数据，我们要使用 对于不重要的数据，我们不太想使用 但是，对于一个模型而言（CNN、LSTM），很难决定什么重要，什么不重要 由此，注意力机制诞生了（有人发现了如何去在深度学习的模型上做注意力）  红色的是科学家们发现，如果给你一张这个图，你眼睛的重点会聚焦在红色区域 人–》看脸 文章看标题 段落看开头 后面的落款 这些红色区域可能包含更多的信息，更重要的信息 注意力机制：我们会把我们的焦点聚焦在比较重要的事物上 怎么做注意力我（查询对象...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/11%20Self-Attention%E7%9B%B8%E6%AF%94%E8%BE%83%20RNN%E5%92%8CLSTM%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9/" title="11 Self-Attention相比较 RNN和LSTM的优缺点">11 Self-Attention相比较 RNN和LSTM的优缺点</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  RNN 无法做长序列，当一段话达到 50 个字，效果很差了 LSTM LSTM 通过各种门，遗忘门，选择性的可以记忆之前的信息（200 词） Self-Attention 和 RNNs 的区别RNNs 长序列依赖问题，无法做并行 Self-Attention 得到的新的词向量具有句法特征和语义特征（词向量的表征更完善） 句法特征  语义特征  并行计算</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/12%20Masked%20Self-Attention%EF%BC%88%E6%8E%A9%E7%A0%81%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%89/" title="12 Masked Self-Attention（掩码自注意力机制）">12 Masked Self-Attention（掩码自注意力机制）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  上节课回顾《Attention is all you need》 Attention  Self-Attention（Self–》自–》QKV 同源）句法结构，语义结构   自注意力机制明确的知道这句话有多少个单词，并且一次性给足，而掩码是分批次给，最后一次才给足 Masked（掩码） Self-Attention–》在自注意力模型上面做了改进为什么要做这个改进：生成模型，生成单词，一个一个生成的 当我们做生成任务的时候，我们也想对生成的这个单词做注意力计算，但是，生成的句子是一个一个单词生成的 I have a dream  I  第一次注意力计算，只有...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/13%20Multi-Head%20Self-Attention%EF%BC%88%E4%BB%8E%E7%A9%BA%E9%97%B4%E8%A7%92%E5%BA%A6%E8%A7%A3%E9%87%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%81%9A%E5%A4%9A%E5%A4%B4%EF%BC%89/" title="13 Multi-Head Self-Attention（从空间角度解释为什么做多头）">13 Multi-Head Self-Attention（从空间角度解释为什么做多头）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  上节课回顾0：40 Attention  Self-AttentionSelf-Attention 其实是 Attention 的一个具体做法 给定一个 X，通过自注意力模型，得到一个 Z，这个 Z 就是对 X 的新的表征（词向量），Z 这个词向量相比较 X 拥有了句法特征和语义特征   Multi-Head Self-Attention（多头自注意力）Z 相比较 X 有了提升，通过 Multi-Head Self-Attention，得到的 $Z{‘}$ 相比较 Z 又有了进一步提升 多头自注意力，问题来了，多头是什么，多头的个数用 h...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/14%20Positional%20Encoding%20%EF%BC%88%E4%B8%BA%E4%BB%80%E4%B9%88%20Self-Attention%20%E9%9C%80%E8%A6%81%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%89/" title="14 Positional Encoding （为什么 Self-Attention 需要位置编码）">14 Positional Encoding （为什么 Self-Attention 需要位置编码）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  厚颜无耻的要个赞Attention优点：  解决了长序列依赖问题 可以并行  缺点：  开销变大了  既然可以并行，也就是说，词与词之间不存在顺序关系（打乱一句话，这句话里的每个词的词向量依然不会变），即无位置关系（既然没有，我就加一个，通过位置编码的形式加）   位置编码的问题 为什么需要位置编码位置编码怎么做的  具体做法做法 1   做法 2   为什么这么做有用pos+K&#x3D;5，我在计算第 5 个单词的位置编码的时候 pos&#x3D;1，k&#x3D;4 pos&#x3D;2，k&#x3D;3 </div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/1401%20%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%85%AC%E5%BC%8F%E8%AF%A6%E7%BB%86%E7%90%86%E8%A7%A3%E8%A1%A5%E5%85%85/" title="1401 位置编码公式详细理解补充">1401 位置编码公式详细理解补充</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  Self-Attention：对于每个词而言都是无位置关系，把每个词的顺序打乱，得到的注意力值依然不变 通过 t1 告诉你，x1 是在前面，x2 在 x1 的后面 位置编码     位置编码公式     位置编码怎么用     位置编码底层解释   sin(pos+k) = sin(pos)*cos(k) + cos(pos)*sin(k)  # sin 表示的是偶数维度cos(pos+k) = cos(pos)cos(k) - sin(pos)*sin(k)  # cos 表示的是奇数维度他特别在 pos+k 是 pos 和 k 的线性组合我爱你...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/15%20Transformer%20%E6%A1%86%E6%9E%B6%E6%A6%82%E8%BF%B0/" title="15 Transformer 框架概述">15 Transformer 框架概述</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  1000*0.04&#x3D;40–&gt;10 5000*0.04&#x3D;200–&gt;20 预训练–》NNLM–》word2Vec–》ELMo–》Attention NLP 中预训练的目的，其实就是为了生成词向量 顺水推舟，transformer 其实就是 attention 的一个堆叠 从一个宏观的角度，去看 transformer...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/07/12/2.Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/16%20Transformer%20%E7%9A%84%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Encodes%EF%BC%89%E2%80%94%E2%80%94%E6%88%91%E5%9C%A8%E5%81%9A%E6%9B%B4%E4%BC%98%E7%A7%80%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F/" title="16 Transformer 的编码器（Encodes）——我在做更优秀的词向量">16 Transformer 的编码器（Encodes）——我在做更优秀的词向量</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-07-12T09:14:18.000Z" title="发表于 2022-07-12 17:14:18">2022-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/">2.Areas🌐</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/">science</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/">Pre</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/2-Areas%F0%9F%8C%90/science/Pre/pytorch%E5%AD%A6%E4%B9%A0/Transformer%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Transformer前世今生</a></span></div><div class="content">博客配套视频链接: https://space.bilibili.com/383551518?spm_id_from=333.1007.0.0  b 站直接看  配套 github 链接：https://github.com/nickchen121/Pre-training-language-model  配套博客链接：https://www.cnblogs.com/nickchen121/p/15105048.html  Transformer 框架seq（编码器）2seq（解码器）  通过编码器对序列进行向量化（词向量） 把词向量输入到解码器，得到结果（生成单词）  编码器概略图  编码器包括两个子层，Self-Attention、Feed Forward 每一个子层的传输过程中都会有一个（残差网络+归一化） 编码器详细图  Thinking –》得到绿色的 x1（词向量，可以通过 one-hot、word2vec 得到）+ 叠加位置编码（给 x1 赋予位置属性）得到黄色的 x1 –》输入到 Self-Attention 子层中，做注意力机制（x1、x2...</div></div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/23/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/#content-inner">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/#content-inner">25</a><a class="extend next" rel="next" href="/page/25/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">yuezi</div><div class="author-info-description">积微者速成</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">603</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">85</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yuezi2048"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://githubfast.com/yuezi2048" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_46345703" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="https://qm.qq.com/cgi-bin/qm/qr?k=dD62GIPTf5-iS4UdSfJRy7NHwsrCh3-j" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:joyLing@stumail.nwu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">积微者速成</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/Excalidraw/Drawing%202024-11-01%2015.37.37.excalidraw/" title="Drawing 2024-11-01 15.37.37.excalidraw">Drawing 2024-11-01 15.37.37.excalidraw</a><time datetime="2025-10-31T04:19:26.621Z" title="发表于 2025-10-31 12:19:26">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/01.project/condefather/yupao/back-end/7.%E4%BC%98%E5%8C%96%E4%B8%BB%E9%A1%B5%E6%80%A7%E8%83%BD-%E4%BC%98%E5%8C%96%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE/" title="7.优化主页性能-优化批量导入数据">7.优化主页性能-优化批量导入数据</a><time datetime="2025-10-31T04:19:26.102Z" title="发表于 2025-10-31 12:19:26">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/02.%E9%9D%A2%E7%BB%8F%E8%AE%B0%E5%BD%95/MySQL/81.%E4%BB%80%E4%B9%88%E6%98%AF%E6%B8%B8%E6%A0%87Cursor%E5%88%86%E9%A1%B5_%E5%AF%B9%E6%AF%94%E4%BC%A0%E7%BB%9FLIMIT_OFFSET%E5%88%86%E9%A1%B5%E7%9A%84%E4%BC%98%E5%8A%BF%E6%98%AF%E4%BB%80%E4%B9%88/" title="81.什么是游标Cursor分页_对比传统LIMIT_OFFSET分页的优势是什么">81.什么是游标Cursor分页_对比传统LIMIT_OFFSET分页的优势是什么</a><time datetime="2025-10-31T04:14:27.000Z" title="发表于 2025-10-31 12:14:27">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/02.%E9%9D%A2%E7%BB%8F%E8%AE%B0%E5%BD%95/MySQL/80.%E5%85%A8%E9%87%8F%E5%90%8C%E6%AD%A5%E5%92%8C%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5%E5%90%84%E8%87%AA%E4%BC%98%E7%BC%BA%E7%82%B9/" title="80.全量同步和增量同步各自优缺点">80.全量同步和增量同步各自优缺点</a><time datetime="2025-10-31T04:13:32.000Z" title="发表于 2025-10-31 12:13:32">2025-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/31/2.Areas%F0%9F%8C%90/02.%E9%9D%A2%E7%BB%8F%E8%AE%B0%E5%BD%95/MySQL/79.%E4%BB%80%E4%B9%88%E6%98%AF%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE%E5%85%A5%E5%BA%93_%E7%9B%B8%E6%AF%94%E5%8D%95%E6%9D%A1%E6%8F%92%E5%85%A5%E4%BC%98%E5%8A%BF/" title="79.什么是批量数据入库_相比单条插入优势">79.什么是批量数据入库_相比单条插入优势</a><time datetime="2025-10-31T04:03:26.000Z" title="发表于 2025-10-31 12:03:26">2025-10-31</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
      <i class="fas fa-angle-right"></i></a>
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/0-InBox%F0%9F%90%AD/"><span class="card-category-list-name">0.InBox🐭</span><span class="card-category-list-count">2</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/0-InBox%F0%9F%90%AD/md/"><span class="card-category-list-name">md</span><span class="card-category-list-count">2</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/2-Areas%F0%9F%8C%90/"><span class="card-category-list-name">2.Areas🌐</span><span class="card-category-list-count">584</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/2-Areas%F0%9F%8C%90/00-%E7%A7%8B%E6%8B%9B%E5%87%86%E5%A4%87%E6%96%B9%E5%90%91/"><span class="card-category-list-name">00.秋招准备方向</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/2-Areas%F0%9F%8C%90/01-project/"><span class="card-category-list-name">01.project</span><span class="card-category-list-count">24</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/2-Areas%F0%9F%8C%90/01-project/condefather/"><span class="card-category-list-name">condefather</span><span class="card-category-list-count">24</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/2-Areas%F0%9F%8C%90/01-project/condefather/user-center/"><span class="card-category-list-name">user-center</span><span class="card-category-list-count">9</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/2-Areas%F0%9F%8C%90/01-project/condefather/user-center/back-end/"><span class="card-category-list-name">back-end</span><span class="card-category-list-count">8</span></a></li></ul></li></ul></li></ul></li></ul></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/6-Excalidraw%F0%9F%8E%A8/" style="font-size: 1.1em; color: #999">6.Excalidraw🎨</a> <a href="/tags/back-end/" style="font-size: 1.37em; color: #99a4b2">back-end</a> <a href="/tags/md/" style="font-size: 1.12em; color: #999a9b">md</a> <a href="/tags/GNN/" style="font-size: 1.1em; color: #999">GNN</a> <a href="/tags/pytorch%E5%9F%BA%E6%9C%AC%E5%8A%9F/" style="font-size: 1.14em; color: #999b9d">pytorch基本功</a> <a href="/tags/Java%E9%9B%86%E5%90%88/" style="font-size: 1.41em; color: #99a5b7">Java集合</a> <a href="/tags/%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/" style="font-size: 1.19em; color: #999da1">基本方法</a> <a href="/tags/%E6%80%BB%E4%BD%93%E8%A7%84%E5%88%92/" style="font-size: 1.12em; color: #999a9b">总体规划</a> <a href="/tags/docker/" style="font-size: 1.1em; color: #999">docker</a> <a href="/tags/Java%E5%9F%BA%E7%A1%80/" style="font-size: 1.5em; color: #99a9bf">Java基础</a> <a href="/tags/4-JavaWeb/" style="font-size: 1.28em; color: #99a0aa">4.JavaWeb</a> <a href="/tags/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/" style="font-size: 1.43em; color: #99a6b9">论文梳理</a> <a href="/tags/yupao/" style="font-size: 1.1em; color: #999">yupao</a> <a href="/tags/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95/" style="font-size: 1.34em; color: #99a3b0">代码随想录</a> <a href="/tags/default/" style="font-size: 1.1em; color: #999">default</a> <a href="/tags/%E5%B0%8F%E8%AE%BA%E6%96%87%E6%95%85%E4%BA%8B%E7%BA%BF%E5%92%8C%E6%A8%A1%E5%9D%97%E8%AE%B0%E5%BD%95%E7%B4%A0%E6%9D%90/" style="font-size: 1.21em; color: #999da4">小论文故事线和模块记录素材</a> <a href="/tags/front-end/" style="font-size: 1.1em; color: #999">front-end</a> <a href="/tags/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%87%86%E5%A4%87/" style="font-size: 1.26em; color: #999fa8">小论文准备</a> <a href="/tags/Redis/" style="font-size: 1.48em; color: #99a8bd">Redis</a> <a href="/tags/00-%E7%A7%8B%E6%8B%9B%E5%87%86%E5%A4%87%E6%96%B9%E5%90%91/" style="font-size: 1.17em; color: #999c9f">00.秋招准备方向</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E7%AF%87%E2%80%94%E2%80%94%E9%9F%A9%E9%A1%BA%E5%B9%B3/" style="font-size: 1.39em; color: #99a5b4">基础篇——韩顺平</a> <a href="/tags/Awesome-LLM-for-RecSys-main/" style="font-size: 1.1em; color: #999">Awesome-LLM-for-RecSys-main</a> <a href="/tags/%E6%97%A9%E6%9C%9F%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 1.1em; color: #999">早期工作的基础知识</a> <a href="/tags/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%EF%BC%9AMENTER-%E7%94%A8%E6%88%B7%E6%B3%A8%E5%85%A5-%E5%9B%A0%E6%9E%9C%E6%AD%A3%E5%88%99/" style="font-size: 1.32em; color: #99a2ae">第一次实验：MENTER+用户注入+因果正则</a> <a href="/tags/Spring/" style="font-size: 1.17em; color: #999c9f">Spring</a> <a href="/tags/1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 1.26em; color: #999fa8">1.监督学习</a> <a href="/tags/Spring-MVC/" style="font-size: 1.3em; color: #99a1ac">Spring MVC</a> <a href="/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/" style="font-size: 1.12em; color: #999a9b">算法基础课</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/" style="font-size: 1.1em; color: #999">模型整理</a> <a href="/tags/2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 1.14em; color: #999b9d">2.无监督学习</a> <a href="/tags/vue/" style="font-size: 1.12em; color: #999a9b">vue</a> <a href="/tags/7-SpringBoot/" style="font-size: 1.23em; color: #999ea6">7.SpringBoot</a> <a href="/tags/JVM/" style="font-size: 1.46em; color: #99a7bb">JVM</a> <a href="/tags/Github/" style="font-size: 1.12em; color: #999a9b">Github</a> <a href="/tags/Excalidraw/" style="font-size: 1.1em; color: #999">Excalidraw</a> <a href="/tags/SSM%E6%95%B4%E5%90%88/" style="font-size: 1.1em; color: #999">SSM整合</a> <a href="/tags/5-Maven%E4%B8%93%E9%A2%98/" style="font-size: 1.12em; color: #999a9b">5.Maven专题</a> <a href="/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%80%BB%E7%BB%93/" style="font-size: 1.12em; color: #999a9b">论文复现总结</a> <a href="/tags/3-JDBC/" style="font-size: 1.14em; color: #999b9d">3.JDBC</a> <a href="/tags/%E5%8F%AF%E8%83%BD%E4%BC%9A%E7%94%A8%E5%88%B0%E7%9A%84%E6%A8%A1%E5%9D%97/" style="font-size: 1.14em; color: #999b9d">可能会用到的模块</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      <a class="card-more-btn" href="/archives/"
            title="查看更多">
            <i class="fas fa-angle-right"></i>
          </a>
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">201</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/09/">
            <span class="card-archive-list-date">
              九月 2025
            </span>
            <span class="card-archive-list-count">98</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">39</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/06/">
            <span class="card-archive-list-date">
              六月 2025
            </span>
            <span class="card-archive-list-count">24</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/05/">
            <span class="card-archive-list-date">
              五月 2025
            </span>
            <span class="card-archive-list-count">13</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/04/">
            <span class="card-archive-list-date">
              四月 2025
            </span>
            <span class="card-archive-list-count">80</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">6</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">603</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-04-05T00:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">1041.3k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-10-31T04:19:46.697Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/backgroud.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By yuezi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.0.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: str => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: subtitleType => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        btf.getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  },
  processSubtitle: (content, extraContents = []) => {
    if (true) {
      const sub = [].slice()

      if (extraContents.length > 0) {
        sub.unshift(...extraContents)
      }

      if (typeof content === 'string') {
        sub.unshift(content)
      } else if (Array.isArray(content)) {
        sub.unshift(...content)
      }

      sub.length > 0 && typedJSFn.init(sub)
    } else {
      document.getElementById('subtitle').textContent = typeof content === 'string' ? content :
        (Array.isArray(content) && content.length > 0 ? content[0] : '')
    }
  }
}
btf.addGlobalFn('pjaxSendOnce', () => { typed.destroy() }, 'typedDestroy')
</script><script>function subtitleType () {
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      const from = '出自 ' + data.from
      typedJSFn.processSubtitle(data.hitokoto, [from])
    })
    .catch(err => {
      console.error('Failed to get the Hitokoto API:', err)
      typedJSFn.processSubtitle([])
    })
}
typedJSFn.run(subtitleType)
</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>